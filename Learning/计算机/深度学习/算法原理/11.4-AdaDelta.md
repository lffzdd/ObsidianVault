>[AdaDelta — PaddleEdu documentation](https://paddlepedia.readthedocs.io/en/latest/tutorials/deep_learning/optimizers/adadelta.html)
>[自适应学习率调整：AdaDelta - Physcal - 博客园](https://www.cnblogs.com/neopenx/p/4768388.html)



由于AdaGrad单调递减的学习率变化过于激进，考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。Adadelta是 Adagrad 的扩展，旨在帮助缓解后者学习率单调下降的问题。

指数移动平均值大约就是过去一段时间的**平均值**（期望也算是一种平均值），因此我们用这一方法来计算二阶累积动量：
$$\Delta\theta_t=-\eta\cdot g_{t,i}\\\theta_{t+1}=\theta_t+\Delta\theta_t$$
$$E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma)g_t^2$$
其中$\gamma$类似于冲量，大约是0.9.现在将SGD更新的参数变化向量$\Delta\theta_t{:}$
在Adagrad中，$\Delta\theta_t$是由：
$$\Delta\theta_t=-\frac\eta{\sqrt{G_t+\epsilon}}\cdot g_{t,i}$$
表示的，现在用$E[g^2]_t$简单代替原来的对角矩阵$G_t:$
$$\Delta\theta_t=-\frac\eta{\sqrt{E[g^2]_t+\epsilon}}\cdot g_{t,i}$$
则
$$
\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{\gamma\cdot\frac{1}{t-1}(g_{1}^2+g_{2}^2+\cdots+g_{t-1}^{2})+(1-\gamma)g_{t}^{2}+\epsilon}}\cdot g_{t,i}
$$
将分母简记为RMS，表示梯度的均方根误差：
$$\Delta\theta_t=-\frac\eta{RMS[g]_t}\cdot g_t$$
根据作者所说，更新中，定义指数衰减均值，代替梯度平方：
$$E[\Delta\theta^2]_t=\gamma E[\Delta\theta^2]_{t-1}+(1-\gamma)\Delta\theta_t^2$$
均方根误差变为：
$$RMS[\Delta\theta]_t=\sqrt{E[\Delta\theta^2]_t+\epsilon}$$
$RMS[\Delta\theta]_t$ 是未知的，我们近似用前一个时间步 RMS(root mean square)值来估计：
$$\Delta\theta_t=-\frac{RMS[\Delta\theta]_{t-1}}{RMS[g]_t}g_t\theta_{t+1}=\theta_t-\Delta\theta_t\Phi$$ 
Adadelta不用设置学习率，因为其更新规则已经把它消除了。

#### 优点
- 避免了二阶动量持续累积、导致训练过程提前结束的问题了