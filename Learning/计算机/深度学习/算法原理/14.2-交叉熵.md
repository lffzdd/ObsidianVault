# 看作不确定性
首先思考一个问题
信息论里面, ==信息量是不确定性的减少==, $-\log p(x)$ 表示不确定性很合适, 事件概率越大, 不确定性越小
但是 $-\log p(x)$ 不合适定义为信息量, 不然, 事件概率越大, 你拥有的信息量应该越多才是, 而 $-\log p(x)$ 却变小, 所以, 以后我就应该将 $-\log p(x)$ 看作***不确定性***, 不然容易混淆
# $H(p,q)=-\sum_{x}p(x)\log{q(x)}$
现在, 再来思考这个公式
假设我观测世界, 观测到 $x$ 的发生概率为 $p(x)$, 对我来说, 世界的总不确定性就是 $H(p)=-\sum p(x)\log{p(x)}$
另一个人观测世界, 同理, 观测到的分布为 $q(x)$,世界的总不确定性是 $H(q)=-\sum q(x)\log{q(x)}$
现在再来想想我们是怎么观测的?
我观测了 $N$ 次, 观测到事件 $x_{i}$ 出现了 $m_{i}$ 次, 观测完之后统计结果, 发现事件 $x_{i}$ 的出现概率为 $p(x_{i})=\frac{m_{i}}{N}$, 将所有的结果整合得到总的概率分布 $p(x)$
==我是根据概率来**估计**事件的不确定性==, 概率越大, 不确定性越低, 也就是事件 $x_{i}$ 的不确定性为 $-\log p(x_{i})$. 
接下来, 计算世界的总不确定性, 我每次观测到 $x_{i}$ 有 $m_{i}$ 次, 每次我都加一个不确定性 $-\log p(x_{i})$, 事件 $x_{i}$ 提供的总不确定性就有 $m_{i}\times -\log p(x_{i})$, 这次观测所有事件的总不确定性就为 
$$
\begin{align}
 &m_{1}\times -\log p(x_{1})+m_{2}\times -\log p(x_{2})+\cdots+m_{n}\times -\log p(x_{n})\\
=&\sum m_{i}(-\log p(x_{i}))
\end{align}
$$
若 $N$ 越大, 观测次数越多, 总不确定性就越大, 这明显不合理, 上述式子除以 $N$ 消除观测次数的影响
$$
\begin{align}
H(p)=& \frac{\sum m_{i}(-\log p(x_{i}))}{N}\\
 =& \frac{m_{1}}{N} \times -\log p(x_{1})+ \frac{m_{2}}{N} \times -\log p(x_{2})+\cdots+ \frac{m_{n}}{N} \times -\log p(x_{n})\\
 =&p(x_{1})\times -\log p(x_{1})+p(x_{2})\times -\log p(x_{2})+\cdots+p(x_{n})\times -\log p(x_{n})\\
 =&\sum -p(x)\log p(x)
\end{align}
$$
另一个人通过同样的观测得到了 $q(x)$ 和 $-\log q(x)$ 以及 $H(q)$
于是，对同一个世界，我们根据各自的观测得到了各自的结论 $p$ 和 $q$, 认为世界的总不确定性分别为 $H(p)$ 和 $H(q)$
这时候，我想比较一下我和他对于世界总不确定性的区别
我假设, 我每次观测到事件 $x_{i}$ 时加的一个确定性不为 $-\log p(x_{i})$, 而是 $-\log q(x_{i})$, $x_{i}$ 观测到了 $m_{i}$ 次, 于是有
$$
\begin{align}
 &m_{1}\times -\log q(x_{1})+m_{2}\times -\log q(x_{2})+\cdots+m_{n}\times -\log q(x_{n})\\
=&\sum m_{i}(-\log q(x_{i}))
\end{align}
$$
同样, 除以 $N$ 消除 $N$ 的影响
$$
\begin{align}
H(p,q)=& \frac{\sum m_{i}(-\log q(x_{i}))}{N}\\
 =& \frac{m_{1}}{N} \times -\log q(x_{1})+ \frac{m_{2}}{N} \times -\log q(x_{2})+\cdots+ \frac{m_{n}}{N} \times -\log q(x_{n})\\
 =&p(x_{1})\times -\log q(x_{1})+p(x_{2})\times -\log q(x_{2})+\cdots+p(x_{n})\times -\log q(x_{n})\\
 =&\sum -p(x)\log q(x)
\end{align}
$$
得到了上面这个总不确定性
直到现在, 我也只是 #勉强理解
# 为什么 $H(p,q)\ge H(p)$
你的直觉是正确的，确实当 $p(x) \neq q(x)$ 时，交叉熵 $H(p,q)$ 会比 $H(p)$ 大，尤其是在你用 $p(x)$ 来加权 $\log q(x)$ 时会引入更多的不确定性。这个现象背后有一些数学和信息论的原因。让我详细解释一下。
### 1. **交叉熵的定义**
我们已经知道，交叉熵 $H(p,q)$ 的定义是：
$$H(p, q) = - \sum p(x) \log q(x)$$
它表示的是用分布 $p(x)$ 来表示事件时，预测分布 $q(x)$ 会带来的总不确定性。即，我们用 **自己对世界的理解** $p(x)$，但是实际情况却是另一个分布 $q(x)$。
而 **熵** $H(p)$ 是表示你对自己观测到的世界的不确定性：
$$H(p) = - \sum p(x) \log p(x)$$
### 2. **为什么 $H(p, q) \geq H(p)$？**
核心原因是 **对数函数是凹函数**，并且**加权平均不确定性是最小的**。这里的推导其实可以借助 **Jensen 不等式** 来理解。
#### Jensen 不等式
Jensen 不等式指出，对于一个凹函数 f 和随机变量 X，有：
$$\left( \mathbb{E}[X] \right) \geq \mathbb{E}[f(X)]$$
应用到对数函数上，假设我们有概率分布 $p(x)$ 和另一个分布 $q(x)$，对数函数是一个凹函数：
$$\log \left( \sum_{x} p(x) q(x) \right) \geq \sum_{x} p(x) \log q(x)$$
这意味着如果 $q(x) \neq p(x)$，通过将每个事件的真实概率 $p(x)$ 来加权 $\log q(x)$，你就会得到比 $H(p)$ 更大的值，即 $H(p, q) \geq H(p)$。
#### 直观解释
1. **加权平均和加权对数**：
    - 当你用 $p(x)$ 来加权 $\log q(x)$ 时，你是在对不同事件的 “信息量” 进行加权。如果你对每个事件 $x_i$ 赋予的概率 $p(x_i)$ 不是它的真实概率 $q(x_i)$，你实际上是在使用 **不准确的权重** 来衡量不确定性。
2. **不一致性带来的增大不确定性**：
    - 如果 $p(x)$ 和 $q(x)$ 不一致，意味着你在用自己的理解来推测另一个人的真实世界。因为你对每个事件的不确定性评估方式与真实情况不符，所以这种 “误差” 或者 “不一致性” 必然会导致更大的总不确定性。
    - 数学上讲，**加权的对数和不一致时，结果会比加权真实概率的对数大**，这正是交叉熵的一个重要特性。
3. **最小化交叉熵：**
    - **当 $p(x) = q(x)$ 时，交叉熵 $H(p,q)$ 和熵 $H(p)$ 是相等的**。这时你对事件的理解是准确的，没有信息浪费或不一致，因此不确定性最小。
    - **当 $p(x) \neq q(x)$ 时，交叉熵总是大于熵 $H(p)$**，因为你将自己不准确的预测（$p(x)$）用来衡量真实的不确定性（$q(x)$）。
### 3. **交叉熵与信息量的关系**
在信息论中，交叉熵不仅仅是在比较不确定性，它还反映了**信息量的浪费**。我们可以把它看作是“预测损失”：
- **H(p)** 代表你根据自己的观测对世界的不确定性。
- **H(p, q)** 代表你用自己的观测（$p(x)$）来预测或理解别人的世界（$q(x)$）时，会增加的“信息浪费”。
信息量越少，系统越不确定，而信息浪费越多，交叉熵就越大。**当你尝试通过错误的概率分布去理解世界时，所得到的总信息量会更大，表示你对世界的理解远离了真实的状态。**
### 4. **总结：**
- **熵 $H(p)$**：是你对自己观测到的世界的理解，反映了你自己世界的不确定性。
- **交叉熵 $H(p,q)$**：是你用自己对世界的理解 $p(x)$ 来预测另一种世界 $q(x)$ 时所产生的信息浪费，它总是大于或等于熵 $H(p)$。
- **原因**：因为对数函数是凹函数，使用不一致的概率分布来加权计算会导致更大的不确定性。
因此，交叉熵提供了一个度量，用来衡量当你用不准确的预测（即 $p(x)$）来表示实际分布 $q(x)$ 时所增加的 “信息浪费”。当 $p(x)$ 和 $q(x)$ 完全一致时，这个信息浪费最小，交叉熵和熵相等。
希望这样能更清楚地帮助你理解交叉熵的原因！如果还有疑问，随时告诉我！
# [Site Unreachable](https://zhuanlan.zhihu.com/p/149186719)
我好像明白了, 假设只有两个事件 $x_1,x_2$ 的概率搞反了,其他事情全是预估正确,
对 $x_1$,有 $p(x_1)$ 的概率带来 $-\log p(x_1)$ 的不确定性
对 $x_2$,有 $p(x_2)$ 的概率带来 $-\log p(x_2)$ 的不确定性
那么,真实的这两个事件贡献的总不确定性就是 $p(x_1)(-\log p(x_1))+p(x_2)(-\log p(x_2))$,
预估的为 $p(x_1) (-\log p(x_2))+p(x_2)(-\log p(x_1))$ 
- & 如果 $p(x_1) >p(x_2)$,那么 $x_1$ 的不确定性就更小, $x_2$ 的更大, $p(x_1) (-log p(x_2))$ 更大的概率带来了更大的不确定性, $p(x_2) (-\log p(x_1))$ 更小的概率带来了更小的不确定性,当然总不确定性就变大了
之所以之前没想到,是因为没有把各个事件看待成一组事件,估错一个事件就会导致至少另一个事件的错估
把所有事件的真实概率和预测概率视作一组事件，而其中一个事件的概率被错误估计了，那么会对其他事件的估计产生影响。这是因为交叉熵是衡量预测分布与真实分布之间的差异，并且误估的部分会导致“额外的不确定性”被引入。