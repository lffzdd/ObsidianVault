---
aliases: 残差连接
tags: []
date created: Invalid date
date modified: 2024, 七月 3日, 11:30:59,  星期三中午
---
# 图像的残差
> [ResNet (actually) explained in under 10 minutes - YouTube](https://www.youtube.com/watch?v=o_3mboe1jYI&ab_channel=rupertai)
![[Pasted image 20240702200246.png]]
![[Pasted image 20240702200331.png]]
残差连接（Residual Connection）是一种通过添加直接路径来简化深层神经网络训练的技术。它的核心思想是在网络的某些层中引入跳跃连接，允许梯度直接通过这些连接从较浅的层传播到较深的层，从而缓解梯度消失问题，并且有助于更有效地训练深层神经网络。
理解残差连接（Residual Connection）的关键在于它的核心思想：通过学习残差函数来简化深层神经网络的训练和优化过程。
# 残差连接的基本概念
1. **传统映射 vs. 残差映射**：
   - 在传统的神经网络结构中，每个模块学习将输入映射到某个输出，即 $y = \mathcal{H}(x)$，其中 $\mathcal{H}(x)$ 是模块学习到的映射函数。
   - 在残差连接中，我们不直接学习完整的映射 $\mathcal{H}(x)$，而是学习一个残差函数 $\mathcal{F}(x) = \mathcal{H}(x) - x$，即模块的目标是学习到的是与输入 $x$ 相关的差异或残差。
2. **模块输出的构建**：
   - 模块的输出可以表示为 $y = \mathcal{F}(x) + x$，即将输入 $x$ 和学习到的残差 $\mathcal{F}(x)$ 相加，这个相加操作实现了残差连接。
   - 这种设计的好处在于，如果网络能够学习到一个接近于零的残差 $\mathcal{F}(x) \approx 0$，那么输出 $y$ 就会接近于输入 $x$，这种情况下，网络就会更接近于一个恒等映射，即 $y \approx x$，这种恒等映射可以更容易地学习。
3. **优点与应用**：
   - 残差连接有助于解决深层神经网络训练过程中的梯度消失和梯度爆炸问题，因为它使得信息在网络中更容易地向后传播。
   - 它也使得可以更深的网络结构更容易训练和优化，因为每个模块可以选择性地学习到一个接近于零的残差，而不需要完全重新学习整个映射。
### 示例理解
在残差连接的示例中：
-$\mathcal{H}(x)$ 是模块学习到的映射函数，它会对输入 $x$ 进行特征提取和变换。
-$\mathcal{F}(x) = \mathcal{H}(x) - x$ 是学习到的残差函数，它表示模块学习到的是输入 $x$ 与 $\mathcal{H}(x)$ 之间的差异。
- 输出 $y$ 是将输入 $x$ 和学习到的残差 $\mathcal{F}(x)$ 相加得到的结果，这种相加操作可以帮助网络更有效地学习和优化。
### 残差连接的数学效果
考虑到梯度传播的角度，假设 $\mathcal{H}(x)$ 是网络中某一层的变换，通过反向传播算法更新这一层的权重时，可以得到：
$$\frac{\partial L}{\partial x} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial x}$$
其中，$L$ 是损失函数，$y = \mathcal{H}(x) + x$。由于 $\frac{\partial y}{\partial x} = \frac{\partial \mathcal{H}(x)}{\partial x} + 1$，因此残差连接确保了梯度 $\frac{\partial L}{\partial x}$ 不会变得太小，即使 $\frac{\partial \mathcal{H}(x)}{\partial x}$ 的梯度很小。
### 深入分析
1. **梯度传播**：残差连接允许梯度直接通过跳跃连接流向较早的层，这样可以减少梯度在深层网络中的衰减，有助于更有效地传播梯度。
# 残差块 $\mathcal{F}(x)$ 的参数更新
你说得对，在反向传播过程中，我们实际上需要计算的是损失函数相对于每个参数的梯度，而不是输入 $\mathbf{x}$ 的梯度。对于 $\mathbf{W}_1$ 和 $\mathbf{b}_1$ 等权重和偏置参数，我们需要计算它们的梯度以更新这些参数。我们重新梳理残差块中 $\mathcal{F}(\mathbf{x})$ 的梯度更新过程，并特别关注计算 $\mathbf{h}_1$ 相对于 $\mathbf{W}_1$ 的梯度。
### 残差块结构
假设残差块由两个卷积层和一个短接连接组成：
$$ \mathbf{y} = \mathcal{F}(\mathbf{x}) + \mathbf{x} $$
其中：
$$ \mathcal{F}(\mathbf{x}) = \sigma(\mathbf{W}_2 \cdot \sigma(\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2) $$
$\sigma$ 是激活函数（如 ReLU），$\mathbf{W}_1$ 和 $\mathbf{W}_2$ 是权重矩阵，$\mathbf{b}_1$ 和 $\mathbf{b}_2$ 是偏置。
### 前向传播
在前向传播过程中，输入 $\mathbf{x}$ 通过残差块计算输出 $\mathbf{y}$：
1. 计算第一个卷积层的输出：
$$ \mathbf{h}_1 = \sigma(\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) $$
2. 计算第二个卷积层的输出：
$$ \mathbf{h}_2 = \sigma(\mathbf{W}_2 \cdot \mathbf{h}_1 + \mathbf{b}_2) $$
3. 计算残差块的输出：
$$ \mathbf{y} = \mathbf{h}_2 + \mathbf{x} $$
### 反向传播
在反向传播过程中，我们需要计算损失函数 $\mathcal{L}$ 相对于每个参数的梯度，并更新参数。下面是梯度计算的步骤：
- ! 对损失函数来说，$W$ 和 $b$ 才是自变量，$x$ 是常数
1. 计算损失函数相对于输出 $\mathbf{y}$ 的梯度：
$$ \frac{\partial \mathcal{L}}{\partial \mathbf{y}} $$
2. 由于 $\mathbf{y} = \mathbf{h}_2 + \mathbf{x}$，我们需要分别计算 $\frac{\partial \mathcal{L}}{\partial \mathbf{h}_2}$：
$$ \frac{\partial \mathcal{L}}{\partial \mathbf{h}_2} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}} $$
3. 计算损失函数相对于 $\mathbf{W}_2$ 和 $\mathbf{b}_2$ 的梯度：
$$ \frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} = \left( \frac{\partial \mathcal{L}}{\partial \mathbf{h}_2} \cdot \sigma'(\mathbf{W}_2 \cdot \mathbf{h}_1 + \mathbf{b}_2) \right) \cdot \mathbf{h}_1^\top $$
$$ \frac{\partial \mathcal{L}}{\partial \mathbf{b}_2} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_2} \cdot \sigma'(\mathbf{W}_2 \cdot \mathbf{h}_1 + \mathbf{b}_2) $$
4. 计算 $\mathbf{h}_2$ 相对于 $\mathbf{h}_1$ 的梯度：
$$ \frac{\partial \mathbf{h}_2}{\partial \mathbf{h}_1} = \sigma'(\mathbf{W}_2 \cdot \mathbf{h}_1 + \mathbf{b}_2) \cdot \mathbf{W}_2 $$
5. 计算 $\mathbf{h}_1$ 相对于 $\mathbf{W}_1$ 和 $\mathbf{b}_1$ 的梯度：
$$ \frac{\partial \mathbf{h}_1}{\partial \mathbf{W}_1} = \sigma'(\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) \cdot \mathbf{x}^\top $$
$$ \frac{\partial \mathbf{h}_1}{\partial \mathbf{b}_1} = \sigma'(\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) $$
6. 计算损失函数相对于 $\mathbf{W}_1$ 和 $\mathbf{b}_1$ 的梯度：
$$ \frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} = \left( \frac{\partial \mathcal{L}}{\partial \mathbf{h}_1} \cdot \sigma'(\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) \right) \cdot \mathbf{x}^\top $$
$$ \frac{\partial \mathcal{L}}{\partial \mathbf{b}_1} = \frac{\partial \mathcal{L}}{\partial \mathbf{h}_1} \cdot \sigma'(\mathbf{W}_1 \cdot \mathbf{x} + \mathbf{b}_1) $$
### 梯度更新
1. 更新 $\mathbf{W}_2$ 和 $\mathbf{b}_2$：
$$ \mathbf{W}_2 \leftarrow \mathbf{W}_2 - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{W}_2} $$
$$ \mathbf{b}_2 \leftarrow \mathbf{b}_2 - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{b}_2} $$
2. 更新 $\mathbf{W}_1$ 和 $\mathbf{b}_1$：
$$ \mathbf{W}_1 \leftarrow \mathbf{W}_1 - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{W}_1} $$
$$ \mathbf{b}_1 \leftarrow \mathbf{b}_1 - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{b}_1} $$
### 梯度传递的优点
在残差网络中，由于存在短接连接，梯度可以直接从输出层传递到输入层，减少了梯度消失的问题：
这里用 $X$ 代替 $x$，因为这一层的输入 $x$ 其实是上一层的映射函数
$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial \mathbf{X}} &=\\
&=\frac{\partial \mathcal{L}}{\partial \mathbf{y}}\cdot \frac{\partial \mathbf{y}}{\partial \mathbf{X}}=\frac{\partial \mathcal{L}}{\partial \mathbf{y}}\cdot \frac{\partial \mathbf{(\mathcal{F}(X)+X)}}{\partial \mathbf{X}}\\
&=\frac{\partial \mathcal{L}}{\partial \mathbf{y}}\cdot (\frac{\partial \mathbf{(\mathcal{F}(X))}}{\partial \mathbf{X}}+1)\\
&=\frac{\partial \mathcal{L}}{\partial \mathbf{y}}+\frac{\partial \mathcal{L}}{\partial \mathbf{y}}\cdot\frac{\partial \mathbf{(\mathcal{F}(X))}}{\partial \mathbf{X}}
\end{align}
$$
由于 $\frac{\partial y}{\partial x} = \frac{\partial \mathcal{F}(x)}{\partial x} + 1$，因此残差连接确保了梯度 $\frac{\partial L}{\partial x}$ 不会变得太小，即使 $\frac{\partial \mathcal{F}(x)}{\partial x}$ 的梯度很小。

