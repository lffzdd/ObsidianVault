>[文心一言](https://yiyan.baidu.com/chat/4510245510)

优化器（Optimizer）在深度学习和机器学习中扮演着至关重要的角色。它是用来更新和计算影响模型参数（即网络中的权重和偏置），以最小化（或最大化，但在大多数情况下是最小化）某种损失函数（loss function）的算法。损失函数衡量了模型预测与真实数据之间的差异程度，因此，通过最小化这个损失函数，我们可以使得模型对数据的预测更加准确。
优化器的核心任务是找到一种有效的方法来更新模型的参数，以便在每次迭代中都能使损失函数向最小值方向移动。不同的优化器采用了不同的策略来更新参数，这些策略包括但不限于梯度下降（Gradient Descent）、动量（Momentum）、自适应学习率（如AdaGrad、RMSprop、Adam等）。
以下是几种常见的优化器：
1. **梯度下降（Gradient Descent）**：这是最基本的优化算法，它使用损失函数关于参数的梯度（即偏导数）的负值来更新参数。然而，标准的梯度下降算法在每次迭代时都使用整个数据集来计算梯度，这在处理大数据集时非常低效。因此，人们发展了随机梯度下降（SGD）和小批量梯度下降（Mini-batch Gradient Descent）等变体。
2. **动量（Momentum）**：动量算法通过引入一个“速度”变量来加速SGD在相关方向上的训练，并抑制震荡。它模拟了物理中的动量概念，使得参数更新更加平滑。
3. **AdaGrad**：AdaGrad是一种自适应学习率优化算法，它为每个参数维护一个学习率，并随着训练的进行而逐渐减小。它特别适合于处理稀疏梯度的问题。
4. **RMSprop**：RMSprop是AdaGrad的一种改进，它通过引入一个衰减率（decay rate）来丢弃遥远的历史梯度信息，从而允许学习率在不同的时间尺度上进行调整。
5. **Adam**：Adam（Adaptive Moment Estimation）结合了Momentum和RMSprop的思想，通过计算梯度的一阶矩估计和二阶矩估计来更新参数。它通常被认为是目前深度学习中最有效的优化算法之一。
在选择优化器时，需要根据具体任务、数据集大小和模型复杂度等因素进行考虑。不同的优化器可能在不同的场景下表现出不同的性能。此外，优化器的超参数（如学习率、动量系数、衰减率等）也需要根据具体情况进行调整，以获得最佳的训练效果。
---
# 梯度下降算法背景
>[梯度下降算法 — PaddleEdu documentation](https://paddlepedia.readthedocs.io/en/latest/tutorials/deep_learning/optimizers/gd.html)

SGD 会震荡，碰到