---
aliases: 反向传播算法
tags:
date created: 2024, 六月 19日, 03:56:28,  星期三下午
date modified: 2024, 六月 19日, 04:18:34,  星期三下午
---
> [chatgpt.com/share/9a1270ca-8782-4368-b232-9ecce5b9ac54](https://chatgpt.com/share/9a1270ca-8782-4368-b232-9ecce5b9ac54)
在多层神经网络中，梯度通过反向传播算法从输出层向输入层逐层传递，具体来说是通过链式法则（链式求导）进行计算。让我们详细讨论这一过程的数学原理。
## 链式法则的梯度传递
假设我们有一个简单的神经网络，包含 $L$ 层。每一层（第 $l$ 层）的输出可以表示为：
$$a^{(l)} = f(z^{(l)})$$
其中，$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$，这里 $W^{(l)}$ 和 $b^{(l)}$ 分别是第 $l$ 层的权重矩阵和偏置向量，$a^{(l-1)}$ 是第 $l−1$ 层的激活值，$f$ 是激活函数。
$$
\begin{align}
a^{(1)}&=f^{(1)}(z^{(1)}(a^{(0)}))=f^{(1)}(W^{(1)}a^{(0)}+b^{(1)})\\
a^{(2)}&=f^{(2)}(z^{(2)}(a^{(1)}))=f^{(2)}(z^{(2)}(\left[ f^{(1)}(z^{(1)}(a^{(0)}))\right])\\
\vdots\\
a^{(L)}&=f^{(L)}(z^{(L)}(a^{(L-1)}))=\dots
\end{align}
$$
损失函数 $L(a^{(L)})$ 是输出层与真实标签之间的误差。为了通过反向传播算法更新每一层的权重，需要计算损失函数对每一层权重的梯度：
$$\frac{\partial L}{\partial W^{(l)}}​$$
## 链式法则
链式法则告诉我们，如果一个函数是多个函数的复合，那么它的导数等于这些函数的导数的乘积。对于神经网络中的梯度传递，具体来说是：
$$\frac{\partial L}{\partial W^{(l)}}=\frac{\partial L}{\partial z^{(l)}}\cdot\frac{\partial z^{(l)}}{\partial W^{(l)}}$$
继续展开，可以看到:（$a^{(l)}$ 即 $f^{(l)}$）
$$\frac{\partial L}{\partial z^{(l)}}=\frac{\partial L}{\partial a^{(L)}}\cdot\frac{\partial a^{(L)}}{\partial z^{(L)}}\cdot\frac{\partial z^{(L)}}{\partial a^{(L-1)}}\cdot\cdots\cdot\frac{\partial a^{(l+1)}}{\partial z^{(l+1)}}\cdot\frac{\partial z^{(l+1)}}{\partial a^{(l)}}\cdot\frac{\partial a^{(l)}}{\partial z^{(l)}}$$
我们可以看到，梯度传递过程中每一层的梯度是上一层梯度和当前层激活函数导数的乘积。
```ad-def
title: **cor**
collapse: closed
$$
\begin{align}
\frac{\partial L}{\partial W^{(l-1)}}&=\frac{\partial L}{\partial z^{(l-1)}}\cdot\frac{\partial z^{(l-1)}}{\partial W^{(l-1)}}\\
\frac{\partial L}{\partial z^{(l-1)}}&=\frac{\partial L}{\partial a^{(L)}}\cdot\frac{\partial a^{(L)}}{\partial z^{(L)}}\cdot\frac{\partial z^{(L)}}{\partial a^{(L-1)}}\cdot\cdots\cdot\frac{\partial a^{(l)}}{\partial z^{(l)}}\cdot\left[\frac{\partial z^{(l)}}{\partial a^{(l-1)}}\cdot\frac{\partial a^{(l-1)}}{\partial z^{(l-1)}}\right]\\
&=\frac{\partial L}{\partial z^{(l)}}\cdot\left[\frac{\partial z^{(l)}}{\partial a^{(l-1)}}\cdot\frac{\partial a^{(l-1)}}{\partial z^{(l-1)}}\right]\\
则\\
\frac{\partial L}{\partial W^{(l-1)}}&=\frac{\partial L}{\partial z^{(l)}}\cdot\left[\frac{\partial z^{(l)}}{\partial a^{(l-1)}}\cdot\frac{\partial a^{(l-1)}}{\partial z^{(l-1)}}\right]\cdot\frac{\partial z^{(l-1)}}{\partial W^{(l-1)}}\\
&=\frac{\partial L}{\partial z^{(l)}}\cdot\frac{\partial z^{(l)}}{\partial W^{(l-1)}}=\frac{\partial L}{\partial z^{(l)}}\cdot\frac{\partial z^{(l)}}{\partial W^{(l)}}\cdot\frac{\partial W^{(l)}}{\partial W^{(l-1)}}\\
&=\frac{\partial L}{\partial W^{(l)}}\cdot\frac{\partial W^{(l)}}{\partial W^{(l-1)}}
\end{align}
$$
```
具体来说，对于第 $l$ 层：
$$\frac{\partial L}{\partial z^{(l)}}=\delta^{(l)}$$
这里 $\delta^{(l)}$ 是误差项，定义为:
$$\delta^{(l)}=\frac{\partial L}{\partial a^{(l)}}\cdot f'(z^{(l)})$$
## 梯度消失问题
梯度消失问题发生在深层神经网络中，当梯度通过多层网络传递时，如果激活函数的导数很小（如sigmoid或tanh在接近0或1时的导数），这些小梯度的乘积会导致梯度迅速变小。
以sigmoid激活函数为例：
$$\sigma(x) = \frac{1}{1 + e^{-x}}​$$
其导数为：
$$\sigma'(x) = \sigma(x) (1 - \sigma(x))$$
在接近0或1时，$\sigma'(x)$ 接近于0。例如，$\sigma(10) \approx 1$，$\sigma(-10) \approx 0$，所以在这些区域，$\sigma'(x)$ 非常小。如果有多个层，每一层的激活函数导数都很小，经过链式法则的乘积，梯度会迅速衰减：
$$
\frac{\partial L}{\partial W^{(l)}}=\delta^{(L)}\cdot\prod_{k=l}^{L-1}f^{\prime}(z^{(k)})\cdot\frac{\partial z^{(l)}}{\partial W^{(l)}}
$$
### 数学分析
假设每一层的激活函数导数$f^{\prime}(z^{(k)})$平均为一个很小的数$\epsilon$,并且有$L$层，那么：
$$\prod_{k=l}^{L-1}f'(z^{(k)})\approx\epsilon^{L-l}$$
如果 $L$ 很大，$\epsilon^{L-l}$ 将会非常小，接近于0，这就是梯度消失问题的数学原因。
### 解决方案
1. **ReLU激活函数**：ReLU函数的导数为1（在正值区域），避免了梯度消失问题。
2. **权重初始化**：使用Xavier初始化或He初始化，确保初始权重使得前向和反向信号都保持适当的尺度。
3. **批归一化**：在每一层之后进行归一化，减小内部协变量偏移，稳定训练过程。
4. **残差连接**：通过添加跳跃连接，使梯度能够更直接地传递到早期层。
通过这些方法，可以有效减轻或解决梯度消失问题，使得深层神经网络的训练变得更加可行和高效。