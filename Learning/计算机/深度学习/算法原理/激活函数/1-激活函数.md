---
aliases: 1-激活函数
tags:
date created: Wednesday, September 11th 2024, 8:50:24 am
date modified: Tuesday, October 15th 2024, 1:06:04 pm
---
# Softmax
给定一个输入向量$\mathbf{z} = [z_1, z_2, \ldots, z_n]$，Softmax 函数的输出$\mathbf{y} = [y_1, y_2, \ldots, y_n]$计算如下：
$$
\frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$
### Softmax 函数的性质
1. **概率分布**：
    - Softmax 函数的输出值介于 0 和 1 之间，并且所有输出值的总和为 1。
    - 因此，Softmax 函数的输出可以解释为一个概率分布，表示每个类别的概率。
2. **指数放大**：
    - 通过指数运算 ezie^{z_i}ezi​，Softmax 函数可以放大输入向量中的差异，使得最大的输入值对应的输出概率最大。
3. **归一化**：
    - Softmax 函数将任意实数值转换为一个概率分布，确保输出值可以直接用作分类任务中的类别概率。
- 通过指数运算$e^{z_i}$，Softmax 函数可以放大输入向量中的差异，使得最大的输入值对应的输出概率
# ReLU
ReLU（Rectified Linear Unit）激活函数是目前深度学习中最常用的一种激活函数。它具有计算简单、收敛速度快等优点，在许多深度神经网络模型中表现优异。
### ReLU 激活函数的定义
ReLU 激活函数的数学定义如下：
$$\text{ReLU}(x) = \max(0, x)$$
这意味着，如果输入值$x$小于 0，输出为 0；否则，输出为$x$。
### ReLU 的特性和优点
1. **稀疏激活**：
   - ReLU 的一个显著特性是它能够产生稀疏激活。这意味着在同一层的许多神经元在任何给定时间都会输出 0，从而在训练过程中引入了一定的正则化效果。
2. **计算简单**：
   - ReLU 只需要一个简单的阈值操作（比较和取最大值），计算复杂度非常低，显著加快了训练过程。
3. **梯度消失问题的缓解**：
   - 与 sigmoid 或 tanh 激活函数相比，ReLU 能有效缓解梯度消失问题，因为它在正区间的梯度始终为 1，不会像 sigmoid 或 tanh 那样在极端值时梯度趋近于 0。
### ReLU 的缺点
1. **死亡 ReLU（Dying ReLU）问题**：
   - 在训练过程中，如果某些神经元的权重更新使得输入值总是小于 0，那么这些神经元将永远输出 0，从而 " 死亡 "。这会导致这些神经元永不再被激活，对模型的学习能力造成影响。
2. **输出不对称**：
   - ReLU 的输出并不对称，即它只对正值有效。这种不对称性在某些情况下可能会导致模型收敛速度变慢或不稳定。
### ReLU 的变种
为了克服 ReLU 的一些缺点，研究人员提出了几种 ReLU 的变种：
1. **Leaky ReLU**：
   - Leaky ReLU 引入了一个小的负斜率，使得在输入值为负时也有一个小的输出。
   - 定义为：
    $$
     \text{Leaky ReLU}(x) = 
     \begin{cases} 
     x & \text{if } x \geq 0 \\
     \alpha x & \text{if } x < 0 
     \end{cases}
    $$
     其中，$\alpha$是一个很小的常数，通常设为 0.01。
2. **Parametric ReLU (PReLU)**：
   - PReLU 是 Leaky ReLU 的改进版本，其中负斜率$\alpha$是可学习的参数。
   - 定义为：
    $$
     \text{PReLU}(x) = 
     \begin{cases} 
     x & \text{if } x \geq 0 \\
     \alpha x & \text{if } x < 0 
     \end{cases}
    $$
3. **Exponential Linear Unit (ELU)**：
   - ELU 在负区间引入了指数函数，使得输出具有更好的对称性和连续性。
   - 定义为：
    $$
     \text{ELU}(x) = 
     \begin{cases} 
     x & \text{if } x \geq 0 \\
     \alpha (e^x - 1) & \text{if } x < 0 
     \end{cases}
    $$
     其中，$\alpha$是一个超参数。
### 在 Keras 中使用 ReLU
在 Keras 中，可以通过 `Activation` 层或直接在 `Dense` 层等层的 `activation` 参数中使用 ReLU 或其变种。例如：
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
model = Sequential([
    Dense(64, input_dim=100),
    Activation('relu'),  # 使用ReLU激活函数
    Dense(10),
    Activation('softmax')
])
# 或者在Dense层中直接指定激活函数
model = Sequential([
    Dense(64, activation='relu', input_dim=100),
    Dense(10, activation='softmax')
])
```
### 结论
ReLU 激活函数因其简单高效的计算方式，以及在实践中对深度神经网络训练的显著改善，而被广泛应用。尽管存在一些缺点，如 " 死亡 ReLU" 问题，但通过使用其变种（如 Leaky ReLU、PReLU、ELU 等），这些问题可以得到一定程度的缓解。在实际应用中，根据具体的需求和数据特点，选择适合的激活函数至关重要。
### 常见的损失函数
以下是一些常见的损失函数，通常与 ReLU 激活函数一起使用：
#### 1. 均方误差（Mean Squared Error, MSE）
用于回归任务，计算预测值和真实值之间的平方误差的平均值。
公式：
$$\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$
其中，$N$是样本数量，$y_i$是真实值，$\hat{y}_i$是预测值。
#### 2. 平均绝对误差（Mean Absolute Error, MAE）
也用于回归任务，计算预测值和真实值之间的绝对误差的平均值。
公式：
$$\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i|$$
#### 3. 交叉熵损失（Cross-Entropy Loss）
用于分类任务，特别是二分类和多分类问题。二分类交叉熵损失函数公式如下：
$$\text{Binary Cross-Entropy Loss} = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]$$
对于多分类问题，使用分类交叉熵损失：
$$\text{Categorical Cross-Entropy Loss} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})$$
其中，$C$是类别数，$y_{i,c}$是样本$i$在类别$c$上的真实标签，$\hat{y}_{i,c}$是样本$i$在类别$c$上的预测概率。
### 损失函数与 ReLU 的结合
在深度学习模型中，ReLU 激活函数通常与上述损失函数结合使用，具体取决于任务的类型（回归或分类）。例如，在回归任务中，可以使用 MSE 或 MAE 作为损失函数；在分类任务中，可以使用交叉熵损失。
### 示例代码
下面是一个简单的 Keras 示例，展示了如何在使用 ReLU 激活函数的同时选择合适的损失函数：
#### 回归任务
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
# 构建模型
model = Sequential([
    Dense(64, activation='relu', input_dim=100),
    Dense(64, activation='relu'),
    Dense(1)  # 输出层，没有激活函数
])
# 编译模型，使用均方误差作为损失函数
model.compile(optimizer=Adam(), loss='mean_squared_error')
# 训练模型
model.fit(X_train, y_train, epochs=10, batch_size=32)
```
#### 分类任务
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
# 构建模型
model = Sequential([
    Dense(64, activation='relu', input_dim=100),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')  # 输出层，使用softmax激活函数
])
# 编译模型，使用分类交叉熵作为损失函数
model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
# 训练模型
model.fit(X_train, y_train, epochs=10, batch_size=32)
```
### 总结
ReLU 是一个常用的激活函数，用于增加模型的非线性能力，而损失函数用于衡量模型的预测误差。根据具体任务的需求（回归或分类），可以选择合适的损失函数与 ReLU 激活函数结合使用，以优化模型的性能。