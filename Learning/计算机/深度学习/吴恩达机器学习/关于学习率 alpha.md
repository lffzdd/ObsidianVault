一般是使用 $\eta$ 表示学习率，因此我们使用 $\eta$
由之前知道，当斜率 $\theta$ 为某个值时，$J(\theta)$ 最小。假设 $\theta$ 为 1.5 时 $J$ 最小，假设 $\theta$ 的函数如图：
![[Pasted image 20240604151057.png]]
其中 $C$ 的斜率为 1.5，$A$ 的斜率为 1，$B$ 的斜率为 2，设初始值为 1，即 $A$ 点，当 $\eta=1$ 时，梯度下降算法会使得对应的结果从 $A$ 点调到 $B$ 点，又从 $B$ 点跳回 $A$ 点，又从 $A$ 点跳回 $B$ 点，又从……如此反复横跳，所以这种情况只有当 $\eta<1$ 时才能调到 $A$ 和 $B$ 之间，越小就越能逼近 $C$ 点，当然迭代次数也就越大；而如果 $\eta>1$，那就回从 $A$ 点直接跳到 $B$ 点上面，又跳回到 $A$ 点下面，又跳到 $B$ 点更上面，又……每一次迭代，成本函数 $J$ 都会增大，南辕北辙，永远逼近不了 $C$ 点。