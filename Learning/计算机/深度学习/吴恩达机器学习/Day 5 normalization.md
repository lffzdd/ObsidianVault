---
aliases: Day 5 normalization
tags: []
date created: Invalid date
date modified: 2024, 六月 19日, 11:28:56,  星期三上午
---
#### Normal Equation（正规方程）
  - Only for linear regression
  - Solve for w, b without iterations
  - ! Disadvantages:
	  - Doesn't generalize(推广) to other learning algorithms.
	  - Slow when number of features is large (> 10,000)
# 归一化
## Min-Max normalization
设 $X_{Min}$ 和 $X_{Max}$ 分别是属性 A 的最小值和最大值，将 $A$ 的一个原始值 $X$ 通过该标准化方法映射到 $[0,1]$ 区间中得到值 $x$，公式为：
$$x =\frac{X -X_{Min}}{X_{Max} -X_{Min}}$$
如果要将该值 $X$ 映射到 $[a,b]$ 区间内，则公式为
$$x=a+\frac{(X-X_{Min})(b-a)}{X_{Max}-X_{Min}}$$
## Mean normalization
1. 算出平均值 $\mu_i$ ,
2. $x_i=\frac{x_i-\mu_i}{range}$
# 标准化
## Z-score normalization
1. 算出每个特征的标准差 (standard deviation) $\sigma_i$，
2. $x_i=\frac{x_i-\mu_i}{\sigma_i}$
- ! 归一化和标准化的区别：
- 归一化是将样本的特征值转换到同一量纲下（把数据映射到固定的区间内）。该方法仅由变量的极值决定。
- 标准化是按照特征矩阵的列处理数据，通过 Z-score 方法，将样本特征值转换为标准正态分布。该方法和整体样本的分布相关，每一个样本点都能对标准化产生影响。
# 改进性能的数学解释
假设我们有两个特征 $x_1$ ​ 和 $x_2$ ​，它们的**取值范围**差异很大。我们使用梯度下降来最小化损失函数 $J(\mathbf{w})$。
在未归一化的情况下，损失函数的等高线可能非常狭长，形状类似于椭圆。这意味着在某个方向上梯度变化非常大，而在另一个方向上梯度变化非常小，如下图所示：
```lua
  y
  ^
  |   .---.
  | /       \
  | |       |
  | \       /
  |   '---'
  +------------> x
```
在这种情况下，梯度下降算法可能需要很多步才能达到最优点，因为在每一步中都需要非常小心地调整步长，以避免在一个方向上震荡。
而在归一化后，特征 $x_1$ ​ 和 $x_2$ ​ 被缩放到相似的范围，等高线变得更加圆滑和对称，如下图所示：
```lua
  y
  ^
  |   .---.
  |  /     \
  | |       |
  |  \     /
  |   '---'
  +------------> x
```
这使得梯度下降算法能够更加迅速和稳定地收敛到最优点。
### 4. 示例说明
#### 未归一化的情况：
假设两个特征 $x_1$ 和 $x_2$ 的取值范围分别是 [0, 1000] 和 [0, 1]。在梯度下降过程中，由于 $x_1$ 的值远大于 $x_2$，它对损失函数的影响也远大于 $x_2$，这会导致梯度下降算法过度关注 $x_1$ 而忽略 $x_2$。
#### 归一化后的情况：
将 $x_1$ 和 $x_2$ 都缩放到 [0, 1] 范围。这样，两个特征在梯度下降过程中对损失函数的影响是均等的。梯度下降算法能够平等地考虑所有特征，从而更加高效地找到最优解。
