---
aliases: 8.1-转置卷积
tags: []
date created: 星期二, 七月 23日 2024, 12:18:12 中午
date modified: 星期三, 七月 24日 2024, 1:50:04 凌晨
---

# 前言
转置卷积（Transposed Convolution），也被称为反卷积，其数学计算原理可以通过具体的数学公式和步骤来详细解释。以下是一个结合数学公式的说明：
### 一、基本概念
转置卷积的主要目的是在卷积操作的基础上，通过某种方式 " 反转 " 或 " 增大 " 特征图的尺寸。这通常涉及到填充（padding）、步长（stride）和卷积核（kernel）大小等参数。
### 二、计算步骤与公式
#### 1. 标准卷积的输出尺寸公式
首先，我们需要了解标准卷积操作后输出特征图的尺寸计算公式。对于二维卷积，输出尺寸 `n_out` 可以表示为：
- ! $$
n\_{\text{out}} = \left\lfloor \frac{n\_{\text{in}} + 2p - k}{s} \right\rfloor + 1= \left\lfloor\frac{n\_{\text{in}} + 2p - k}{s} + 1\right\rfloor
$$
其中：
- $n\_{\text{in}}$ 是输入特征图的尺寸（假设输入特征图是正方形，即宽度和高度相等）。
- $p$ 是填充（padding）的大小。
- $k$ 是卷积核（kernel）的尺寸。
- $s$ 是步长（stride）。
- $\left\lfloor \cdot \right\rfloor$ 表示向下取整。
- ! 想象一下从后往前移，前 $k$ 个区域后的区域从后往前能移 $\left\lfloor \frac{n+2p-k}{s} \right\rfloor$ 次，接下来最后一步移到前 $k$ 个区域，填满了就是除尽，没填满就是向下取整
```ad-col2
![[Pasted image 20240723134920.png]]

![[Pasted image 20240723135432.png]]
```
# 转置卷积（Transpose Convolution

> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码，原文地址 [paddlepedia.readthedocs.io](https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Transpose_Convolution.html#id4)
一、转置卷积提出背景 [¶]( #id1 "Permalink to this headline")

通常情况下，对图像进行卷积运算时，经过多层的卷积运算后，输出图像的尺寸会变得很小，即图像被约减。而对于某些特定的任务（比如：图像分割、GAN），我们需要将图像恢复到原来的尺寸再进行进一步的计算。这个恢复图像尺寸，实现图像由小分辨率到大分辨率映射的操作，叫做上采样（Upsample），如 **图 1** 所示。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Upsample.png)
图 1 上采样示例
上采样有多种方式，常见的包括：最近邻插值（Nearest neighbor interpolation）、双线性插值（Bi-Linear interpolation）等，但是这些上采样方法都是基于人们的先验经验来设计的，对于很多场景效果并不理想。因此，我们希望让神经网络自己学习如何更好地进行插值，这也就是接下来要介绍的转置卷积（Transpose Convolution）的方法。
二、转置卷积及其应用 [¶]( #id2 "Permalink to this headline")
-------------------------------------------------
转置卷积（Transpose Convolution），在某些文献中也被称为反卷积（Deconvolution）。转置卷积中，不会使用预先设定的插值方法，它具有可学习的参数，通过让网络自行学习，来获取最优的上采样方式。转置卷积在某些特定的领域有着非常广泛的应用，比如：
- 在 DCGAN[1]，生成器将会用随机值转变为一个全尺寸 (full-size) 的图片，这个时候就需要用到转置卷积。
- 在语义分割中，会使用卷积层在编码器中进行特征提取，然后在解码层中进行恢复为原先的尺寸，这样才可以对原来图像的每个像素都进行分类。这个过程同样需要用到转置卷积。经典方法如：FCN[2] 和 Unet[3]。
- CNN 的可视化 [4]：通过转置卷积将 CNN 中得到的特征图还原到像素空间，以观察特定的特征图对哪些模式的图像敏感。
三、转置卷积与标准卷积的区别 [¶]( #id3 "Permalink to this headline")
-----------------------------------------------------
标准卷积的运算操作其实就是对卷积核中的元素与输入矩阵上对应位置的元素进行逐像素的乘积并求和。然后使用卷积核在输入矩阵上以步长为单位进行滑动，直到遍历完输入矩阵的所有位置。
这里举一个简单的例子演示一下具体的操作过程。假设输入是一个 $4\times{4}$ 的矩阵，使用 $3\times{3}$ 的标准卷积进行计算，同时不使用填充，步长设置为 1。最终输出的结果应该是一个 $2\times{2}$ 的矩阵，如 **图 2** 所示。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Standard_Convolution_Example.png)
图 2 标准卷积运算示例
在上边的例子中，输入矩阵右上角 $3\times{3}$ 的值会影响输出矩阵中右上角的值，这其实也就对应了标准卷积中感受野的概念。所以，我们可以说 $3\times{3}$ 的标准卷积核建立了输入矩阵中 9 个值与输出矩阵中 1 个值的对应关系。
综上所述，我们也就可以认为标准卷积操作实际上就是建立了一个多对一的关系。
对于转置卷积而言，我们实际上是想建立一个逆向操作，也就是建立一个一对多的关系。对于上边的例子，我们想要建立的其实是输出卷积中的 1 个值与输入卷积中的 9 个值的关系，如 **图 3** 所示。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Inverse_Convolution_Example.png)
图 3 卷积逆向运算示例
当然，从信息论的角度，卷积操作是不可逆的，**所以转置卷积并不是使用输出矩阵和卷积核计算原始的输入矩阵，而是计算得到保持了相对位置关系的矩阵**。
四、转置卷积数学推导 [¶]( #id4 "Permalink to this headline")
-------------------------------------------------
定义一个尺寸为 $4\times{4}$ 的输入矩阵 $input$:
$$\begin{split} input=\left[\begin{array}{ccc} x_1 & x_2 & x_3 & x_4 \\ x_6 & x_7 & x_8 & x_9 \\ x_{10} & x_{11} & x_{12} & x_{13} \\ x_{14} & x_{15} & x_{16} & x_{17} \end{array}\right] \end{split}$$
一个尺寸为 $3\times{3}$ 的标准卷积核 $kernel$:
$$\begin{split} kernel=\left[\begin{array}{ccc} w_{0,0} & w_{0,1} & w_{0,2} \\ w_{1,0} & w_{1,1} & w_{1,2} \\ w_{2,0} & w_{2,1} & w_{2,2} \end{array}\right] \end{split}$$
令步长 $stride=1$，填充 $padding=0$，按照输出特征图的计算方式 $o = \frac{i + 2p - k}{s} + 1$，我们可以得到尺寸为 $2\times{2}$ 的输出矩阵 $output$ ：
$$\begin{split} output=\left[\begin{array}{ccc} y_0 & y_1 \\ y_2 & y_3 \end{array}\right] \end{split}$$
这里，我们换一个表达方式，我们将输入矩阵 $input$ 和输出矩阵 $output$ 展开成列向量 $X$ 和列向量 $Y$ ，那么向量 $X$ 和向量 $Y$ 的尺寸就分别是 $16\times{1}$ 和 $4\times{1}$，可以分别用如下公式表示：
$$\begin{split} X=\left[\begin{array}{ccc} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_6 \\ x_7 \\ x_8 \\ x_9 \\ x_{10} \\ x_{11} \\ x_{12} \\ x_{13} \\ x_{14} \\ x_{15} \\ x_{16} \\ x_{17} \end{array}\right] \end{split}$$
$$\begin{split} Y=\left[\begin{array}{ccc} y_0 \\ y_1 \\ y_2 \\ y_3 \end{array}\right] \end{split}$$
我们再用矩阵运算来描述标准卷积运算，这里使用矩阵 $C$ 来表示新的卷积核矩阵：
$$Y = CX$$
经过推导，我们可以得到这个稀疏矩阵 $C$，它的尺寸为 $4\times{16}$：
$$\begin{split} { C=\left[\begin{array}{ccc} w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0 \\ 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 \\ 0 & 0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} \end{array}\right] } \end{split}$$
这里，我们用 **图 4** 为大家直观的展示一下上边的矩阵运算过程。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Standard_Convolution_Matrix.png)
图 4 标准卷积矩阵运算示例
而转置卷积其实就是要对这个过程进行逆运算，即通过 $C$ 和 $Y$ 得到 $X$ ：
$$X = C^TY$$
此时，新的稀疏矩阵就变成了尺寸为 $16\times{4}$ 的 $C^T$，这里我们通过 **图 5** 为大家直观展示一下转置后的卷积矩阵运算示例。这里，用来进行转置卷积的权重矩阵不一定来自于原卷积矩阵. 只是权重矩阵的形状和转置后的卷积矩阵相同。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Inverse_Convolution_Matrix.png)
图 5 转置后卷积矩阵运算示例
我们再将 $16\times{1}$ 的输出结果进行重新排序，这样就可以通过尺寸为 $2\times{2}$ 的输入矩阵得到尺寸为 $4\times{4}$ 的输出矩阵了。
五、转置卷积输出特征图尺寸 [¶]( #id5 "Permalink to this headline")
----------------------------------------------------
- **stride=1 的转置卷积**
我们同样使用上文中的卷积核矩阵 $C$：
$$\begin{split} { C=\left[\begin{array}{ccc}w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 & 0 \\0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 & 0 & 0 & 0 \\0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} & 0 \\0 & 0 & 0 & 0 & 0 & w_{0,0} & w_{0,1} & w_{0,2} & 0 & w_{1,0} & w_{1,1} & w_{1,2} & 0 & w_{2,0} & w_{2,1} & w_{2,2} \end{array}\right] } \end{split}$$
对应的输出矩阵 $output$ 为 ：
$$\begin{split} output=\left[\begin{array}{ccc}y_0 & y_1 \\y_2 & y_3\end{array}\right] \end{split}$$
我们将输出矩阵展开为列向量 $Y$ ：
$$\begin{split} Y=\left[\begin{array}{ccc}y_0 \\ y_1 \\y_2 \\ y_3\end{array}\right] \end{split}$$
带入到上文中提到的转置卷积计算公式，则转置卷积的计算结果为：
$$\begin{split} { C^Ty'= \left[\begin{array}{ccc} w_{0,0}y_0 & w_{0,1}y_0+w_{0,0}y_1 & w_{0,2}y_0+w_{0,1}y_1 & w_{0,2}y_1 \\ w_{1,0}y_0+w_{0,0}y_2 & w_{1,1}y_0+w_{1,0}y_1+w_{0,1}y_2+w_{0,0}y_3 & w_{1,2}y_0+w_{1,1}y_1+w_{0,2}y_2+w_{0,1}y_3 & w_{1,2}y_1+w_{0,2}y_3 \\ w_{2,0}y_0+w_{1,0}y_2 & w_{2,1}y_0+w_{2,0}y_1+w_{1,1}y_2+w_{1,0}y_3 & w_{2,2}y_0+w_{2,1}y_1+w_{1,2}y_2+w_{1,1}y_3 & w_{2,2}y_1+w_{1,2}y_3 \\ w_{2,0}y_2 & w_{2,1}y_2+w_{2,0}y_3 & w_{2,2}y_2+w_{2,1}y_3 & w_{2,2}y_3 \end{array}\right] } \end{split}$$
这其实就等价于填充 $padding=2$，输入为：
$$\begin{split} input=\left[\begin{array}{ccc} 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & y_0 & y_1 & 0 & 0 \\ 0 & 0 & y_2 & y_3 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \end{array}\right] \end{split}$$
同时，标准卷积核进行转置：
$$\begin{split} kernel‘=\left[\begin{array}{ccc} w_{2,2} & w_{2,1} & w_{2,0} \\ w_{1,2} & w_{1,1} & w_{1,0} \\ w_{0,2} & w_{0,1} & w_{0,0} \end{array}\right] \end{split}$$
之后的标准卷积的结果，运算过程如 **图 6** 所示。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Transpose_Convolution_s1.gif)
图 6 s=1 时，转置卷积运算示例
对于卷积核尺寸为 $k$，步长 $stride=1$，填充 $padding=0$ 的标准卷积，等价的转置卷积在尺寸为 $i'$ 的输入矩阵上进行运算，输出特征图的尺寸 $o'$ 为：
$$o' = i'+(k-1)$$
同时，转置卷积的输入矩阵需要进行 $padding'=k-1$ 的填充。
- **stride>1 的转置卷积**
在实际使用的过程中，我们大多数时候使用的会是 stride>1 的转置卷积，从而获得较大的上采样倍率。这里，我们令输入尺寸为 $5\times{5}$，标准卷积核的设置同上，步长 $stride=2$，填充 $padding=0$，标准卷积运算后，输出尺寸为 $2\times{2}$。
$$\begin{split} Y=\left[\begin{array}{ccc}y_0 \\ y_1 \\y_2 \\ y_3\end{array}\right] \end{split}$$
这里，步长 $stride=2$，转换后的稀疏矩阵尺寸变为 $25\times{4}$，由于矩阵太大这里不展开进行罗列。则转置卷积的结果为：
$$\begin{split} { C^Ty'=\left[\begin{array}{ccc} w_{0,0}y_0 & w_{0,1}y_0 & w_{0,2}y_0+w_{0,0}y_1 & w_{0,1}y_1 & w_{0,2}y_1\\ w_{1,0}y_0 & w_{1,1}y_0 & w_{1,2}y_0+w_{1,0}y_1 & w_{1,1}y_1 & w_{1,2}y_1\\ w_{2,0}y_0+w_{0,0}y_2 & w_{2,1}y_0+w_{0,1}y_2 & w_{2,2}y_0+w_{2,0}y_1+w_{0,2}y_2+w_{0,0}y_3 & w_{2,1}y_1+w_{0,1}y_3 & w_{2,2}y_1+w_{0,2}y_3\\ w_{1,0}y_2 & w_{1,1}y_2 & w_{1,2}y_2+w_{1,0}y_3 & w_{1,1}y_3 & w_{1,2}y_3\\ w_{2,0}y_2 & w_{2,1}y_2 & w_{2,2}y_2+w_{2,0}y_3 & w_{2,1}y_3 & w_{2,2}y_3\\ \end{array}\right] } \end{split}$$
此时，等价于输入矩阵添加了空洞，同时也添加了填充，标准卷积核进行转置之后的运算结果。运算过程如 **图 7** 所示。
![](https://paddlepedia.readthedocs.io/en/latest/_images/Transpose_Convolution_s2.gif)
图 7 s>1 时，转置卷积运算示例
对于卷积核尺寸为 $k$，步长 $stride>1$，填充 $padding=0$ 的标准卷积，等价的转置卷积在尺寸为 $i'$ 的输入矩阵上进行运算，输出特征图的尺寸 $o'$ 为：
$$o' = s(i'-1)+k$$
>这里很好理解，$i'=\frac{n-k}{s}+1$ ，反过来 $n=s(i'-1)+k$
同时，转置卷积的输入矩阵需要进行 $padding'=k-1$ 的填充，相邻元素间的空洞大小为 $s-1$。因此，可以通过控制步长 $stride$ 来控制上采样倍率。
$$
\begin{split} input=\left[\begin{array}{ccc}
0 & 0 & 0 & 0 & 0 & 0 &0\\
0 & 0 & 0 & 0 & 0 & 0 &0\\
0 & 0 & y_0 & 0& y_1 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 &0\\
0 & 0 & y_2 & 0& y_3 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & 0 & 0
\end{array}\right] \end{split}
$$
# 个人总结
## 这样得到的$X$好像只是尺寸上匹配，但是和之前的 $X$ 不一样吧？
转置卷积的目标是将 $ Y $ 逆向投影回一个更大的空间，从而产生一个比 $ Y $ 更大的输出矩阵。假设我们有：
$$ X' = C^T \cdot Y $$
这里 $ C^T $ 是卷积矩阵 $ C $ 的转置。我们将 $ Y $ 中的信息“传播”到 $ X' $ 中。
尽管 $ X' $ 的尺寸与原始输入 $ X $ 相同，但由于转置卷积使用 $ C^T $ 而不是原始的卷积矩阵 $ C $，我们可能会丢失一些原始信息。转置卷积的目的是为了扩大输出而非精确地逆转输入。
### 为什么结果不同？
1. **信息丢失**：由于卷积是一个信息压缩过程，部分信息在标准卷积过程中被丢失。在转置卷积中，我们试图通过有限的信息重建出更大的特征图，因此 $ X' $ 不一定等于原始 $ X $。
2. **矩阵维度**：虽然 $ C^T $ 可以提供与 $ C $ 相同的维度变换，但它们并不对称，尤其在信息内容方面，因为卷积和转置卷积所执行的操作不同。
3. **学习权重**：在实际应用中，转置卷积中的权重通常是通过反向传播学习到的，而不是简单地通过 $ C^T $ 来获得。
### 总结
转置卷积在尺寸上可以达到和原始输入相同，但并不意味着它能完美地恢复原始输入的信息。这也是为什么在深度学习中，我们通常将转置卷积作为解码器的一部分，并通过反向传播训练学习这些权重，以尽可能地重建或生成所需的输出。
## 转置卷积的“转置”体现在哪里呢？
置卷积（Transposed Convolution）中的“转置”体现在其数学实现方式上，尤其是在稀疏矩阵表示和操作的转置上。
## 代码理解
`Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')（tensor）` 所以，这段代码，输入的参数是指正向训练时的卷积核大小和步长以及填充，而它会根据这些参数得到逆运算中的各种参数？同时这种逆运算也相当于：对tensor进行相应大小的填充后，用正向训练中卷积核倒过来后的卷积核对填充图像进行卷积训练？简短回答一下

---
是的，正确理解了。你使用 `Conv2DTranspose` 层时，指定的参数（例如 `kernel_size`、`strides` 和 `padding`）用于确定反向卷积操作的行为。

简要解释如下：

1. **卷积核大小和步长**：参数 `kernel_size` 和 `strides` 指定了转置卷积操作中卷积核的大小和移动步长。在你的例子中，`kernel_size` 是 `(3, 3)`，`strides` 是 `(2, 2)`，这意味着该层会通过应用 $3 \times 3$ 卷积核和步长为2的方式进行上采样，实际效果是将输入的空间尺寸扩大一倍。

2. **填充**：`padding='same'` 选项确保在考虑步长和卷积核大小时，输出的空间尺寸与输入相同。这可能需要在应用转置卷积之前对输入张量进行填充。

3. **逆操作**：转置卷积本质上是通过反转常规卷积的空间降维效果来执行上采样操作。这涉及到扩展输入张量（例如，通过填充或插入零）并应用一个镜像卷积核，这个卷积核来自前向卷积操作中学到的特征。

4. **可学习参数**：在训练期间，`Conv2DTranspose` 层的权重会被调整，以学习如何最好地从上一层的输出中重建或上采样特征，使其尽可能接近目标输出的形状和内容。

简而言之，`Conv2DTranspose` 层使用指定的参数来确定如何反转先前卷积操作的效果，从而使网络能够执行图像上采样、特征图重建或生成建模等任务。
