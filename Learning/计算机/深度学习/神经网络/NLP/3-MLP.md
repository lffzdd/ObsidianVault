---
aliases: 3-MLP
tags: []
date created: 星期三, 八月 14日 2024, 9:42:37 上午
date modified: 星期五, 八月 16日 2024, 2:22:39 下午
---
 
# Transformer 的整体结构
Transformer 模型由多个编码器（Encoder）和解码器（Decoder）层堆叠而成。每一层编码器和解码器都由以下主要部分构成：
1. **多头自注意力机制（Multi-Head Self-Attention）**：这是 Transformer 的核心部分，允许模型对序列中不同位置之间的依赖关系进行建模。每个输入向量通过多个注意力头来计算与序列中其他位置的关系，并生成多个注意力输出，这些输出被组合在一起。
2. **位置前馈网络（Position-wise Feed-Forward Network）**：在多头自注意力机制之后，位置前馈网络对每个位置的表示进行独立的非线性变换。这个部分处理的是每个位置的特征表示，增强模型的表达能力。
3. **跳跃连接（Residual Connection）**：每个子层（包括注意力机制和位置前馈网络）后都有一个跳跃连接，即将子层的输入加到其输出上，然后再通过一个 LayerNorm 层（[[Learning/计算机/深度学习/神经网络/卷积神经网络/网络层/5-LayerNorm层]]）进行归一化。这有助于缓解深度模型中的梯度消失问题，并加速训练。
4. **Layer Normalization**：在跳跃连接后，LayerNorm 层对输出进行归一化处理，以稳定训练过程并提高模型的收敛速度。
## 编码器（Encoder）和解码器（Decoder）结构
- **编码器（Encoder）**：由多个相同的层堆叠而成，每一层包含一个多头自注意力机制和一个位置前馈网络。编码器接收输入序列并生成其表示。
- **解码器（Decoder）**：也由多个相同的层堆叠而成，但与编码器不同，解码器的每一层包含一个额外的多头注意力机制，用于关注编码器生成的表示。此外，解码器层还包括一个掩码自注意力机制，以确保在生成过程中只关注先前的输出。
总的来说，位置前馈网络在 Transformer 中是对序列中每个位置进行独立处理的重要模块，结合注意力机制，它为每个位置的特征提供了更丰富的表示。这种结构使得 Transformer 能够高效地处理序列数据，并广泛应用于自然语言处理和其他任务中。
# FFN
在 Transformer 架构中，多层感知机（MLP, Multi-Layer Perceptron）通常出现在每个 Transformer 层中的位置前馈网络（Position-wise Feed-Forward Network, FFN）部分。这个位置前馈网络是一个非常重要的组成部分，用于对特征进行进一步的非线性变换。
具体来说，在 Transformer 的每一层中，MLP 通常由两个全连接层（也称为密集层）和一个非线性激活函数（通常是 ReLU）组成。它的工作流程可以描述如下：
1. **输入**：假设输入为一个大小为 `d_model` 的向量，`d_model` 是 Transformer 中的嵌入维度。
2. **第一层全连接层**：输入向量通过一个线性变换，被投影到一个更高维度的空间中。这个维度通常是 `d_ff`，其中 `d_ff` 比 `d_model` 大很多（通常是 4 倍左右）。数学表达为：
   $$
   \text{FFN}(x) = \text{ReLU}(xW_1 + b_1)
   $$
   这里，`W_1` 是一个大小为 `d_model \times d_ff` 的权重矩阵，`b_1` 是一个大小为 `d_ff` 的偏置向量。
3. **非线性激活函数**：第一层全连接层的输出会经过一个非线性激活函数，通常是 ReLU。它引入了非线性，使模型能够表示更复杂的函数。
4. **第二层全连接层**：经过 ReLU 激活后的结果再次通过一个线性变换，被投影回原来的维度 `d_model`。数学表达为：
   $$
   \text{FFN}(x) = ( \text{ReLU}(xW_1 + b_1) ) W_2 + b_2
   $$
   这里，`W_2` 是一个大小为 `d_ff \times d_model` 的权重矩阵，`b_2` 是一个大小为 `d_model` 的偏置向量。
5. **输出**：最终的输出仍然是一个大小为 `d_model` 的向量，==这个向量会被加回到输入中（跳跃连接）==，然后传递到 Transformer 层的下一步处理。
这个 MLP 部分的主要作用是对每个位置的特征进行独立的非线性变换，增强模型的表达能力。它不处理序列的顺序，而是对每个输入位置的表示独立地进行处理。
MLP 在 Transformer 中的使用展示了如何在注意力机制处理全局上下文信息的基础上，进一步通过非线性变换来增强每个位置的表示能力。
