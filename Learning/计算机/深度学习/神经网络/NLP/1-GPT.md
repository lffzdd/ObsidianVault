>[【官方双语】GPT是什么？直观解释Transformer | 深度学习第5章\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV13z421U7cs/?spm_id_from=333.337.search-card.all.click&vd_source=56499cc54ebd02db0ac739e485d74801)

`GPT` 全称为 `Generative Pre-trained Transform`，生成式预训练 Transformer
1. `generative`：生成新文本
2. `pre-trained`：预训练”指的是模型经历了从大量数据中学习的过程，“预”字则暗示 模型能针对具体任务通过额外训练来进行微调
3. `Transformer`：一种特殊的神经网络，一种机器学习模型
# 过程
首先，输入内容会被切分为许多小片段，称为 token，在文本中，token 往往是单词或单词片段或其他常见的字符组合，但对于图像或声音来说，token 则代表小块图像或声音片段
每个 token 对应一个向量，旨在设法偏码该片段的含义
![[Pasted image 20240730153636.png]]
如果将向量看作高维空间中的坐标：![[Pasted image 20240730153736.png]]
## 注意力模块
各个向量相互影响，调整参数
## 多层感知机
各个向量不相互影响，对各个向量为一个问题

# GPT-3
## 将单词看为向量
![[Pasted image 20240802163752.png]]
将单词看为向量。
假设初始的 token，即词汇库，有 5 k 个单词，则初始的嵌入矩阵 $W_E$，有 5000 个向量。将你输入的句子分为一个个单词，每个单词都在这 5000 个向量中有对应的向量
$W_E$ 是随机的，将基于数据进行学习。
在训练过程中，模型发现 ![[Pasted image 20240802183855.png]]
 ![[Pasted image 20240802184153.png]]
 一个词的语义决定了它的向量，将语义分为了多个维度，类似于图中这种差向量个人理解为，向量方向是性别的含义，而向量长度是性别差距的大小。向量方向是国度的差异，向量长度是国度差距的大小
 
 两个向量的点积可以被看作是衡量它们对齐程度的一种方法
![[Pasted image 20240802185116.png]]
```ad-col2
![[Pasted image 20240802185233.png]]
![[Pasted image 20240802192601.png]]

![[Pasted image 20240802185257.png]]
![[Pasted image 20240802192619.png]]
```


![[Pasted image 20240802213439.png]]

| Embedding | 12288* 50257 = 617,558,016 |
| --------- | -------------------------- |
|           |                            |

这些单词向量能结合上下文语境，例如，-一个代表 「国王] 的词嵌入向量，可能会被网络中各个模块逐渐拉扯，最终指向一个更具体细致的方向 ：![[Pasted image 20240802214057.png]]
想想你如何理解某个词，它的词义显然会受到上下文语境影响，有时甚至来自很远的上下文  ：![[Pasted image 20240802214245.png]]
目标就是使其能有效结合上下文信息 
要说明的是，在第一步，即根据输入文本创建向量组时：![[Pasted image 20240802214532.png]]
所以最开始，每个向量只能编码单个单词的含义，没有上下文信息。而流经这个网络的主要目标就是使这些向量能获得，比单个词更丰富更具体的含义。这种网络一次只能处理特定数量的向量： ![[Pasted image 20240802214808.png]]
GTP 的**上下文长度**为 2048，因此流经网络的数据有2048列，每列 12000 维，==上下文长度限制了Transformer 在预测下一个词时，能结合的文本量==，这就是为什么有些聊天机器人，在进行长对话时往往会感觉健忘。请记住，目标输出是下一个可能token的概率分布 ：![[Pasted image 20240802215618.png]]
 
这涉及两个步骤，首先用另一个矩阵将上下文中的最后一个向量映射到一个包含50000个值的列表：![[Pasted image 20240802215841.png]]
然后，一个函数将其归一化为概率分布 ![[Pasted image 20240802220116.png]]
只用最后一个嵌入来做预测似乎有点奇怪？毕竟在最后一层中还有成千上万其他的向量 ![[Pasted image 20240802220225.png]]
这是因为，在训练过程中，效率更高的方法是，利用最终层的每一个向量 ![[Pasted image 20240802220344.png]]
## 解嵌入矩阵 $W_U$
![[Pasted image 20240802220450.png]]

和其它权重矩阵一样,它的初始值随机，在训练过程中学习。
解嵌入矩阵的每行对应词汇库中的一词，每列对应一个嵌入维度，它与嵌入矩阵非常相似只是行列对调
![[Pasted image 20240802220733.png]]
# softmax
![[Pasted image 20240802220831.png]]
![[Pasted image 20240802220911.png]]
![[Pasted image 20240802221044.png]]
![[Pasted image 20240802221105.png]]
![[Pasted image 20240802221136.png]]

 ![[Pasted image 20240802221325.png]]
 它使最大值最接近1，而较小的数值则会非常接近0。
 ![[Pasted image 20240802221424.png]]
 ![[Pasted image 20240802221435.png]]
 ![[Pasted image 20240802221446.png]]
 ![[Pasted image 20240802221457.png]]
 ![[Pasted image 20240802221511.png]]
 你会发现如果输入中某一项显著突出， ![[Pasted image 20240802221605.png]]
 
这样从中抽样，几乎只会选到最大的输入值，但它比只选取最大值要柔和。即当有值靠近最大值时，概率分布中也会获得相当大的权重。并随着输入的连续变化，输出也连续变化
在某些情况下，比如ChatGPT利用该分布生成下一词时，可以给这个函数加入一点趣味性 ![[Pasted image 20240802221944.png]]
$T$ 被称为==温度==，因为它与某些热力学方程中温度的作用有些相似。当 $T$ 较大时，会给低值赋予更多权重，使得分布更均匀一些![[Pasted image 20240802222124.png]]
 极端情况下，将T设为零  ![[Pasted image 20240802222326.png]]
 
## $T$ 的作用
![[Pasted image 20240802222507.png]] 得到的结果就是像“金发姑娘”的老套改编
温度越高模型就更愿意选择可能性较低的词，但风险也更大
```ad-col2
![[Pasted image 20240802222636.png]]

![[Pasted image 20240802222712.png]]
```
 
严格来讲， API并不允许你选择大于2的温度。这没有数学依据，只是人为的一个限制（也许是为了避免他们的工具产生过于荒诞的内容）
人们通常将softmax 函数的输出称为概率，输入则称为logits

输入一些词语，所有这些词嵌入都会流经网络，最后再与解嵌入矩阵相乘 ![[Pasted image 20240802223618.png]]
```ad-col2
![[Pasted image 20240802223705.png]]

![[Pasted image 20240802223727.png]]
```


