# Chapter 5
>[【官方双语】GPT是什么？直观解释Transformer | 深度学习第5章\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV13z421U7cs/?vd_source=56499cc54ebd02db0ac739e485d74801)

![[Pasted image 20241022134611.png]]
这里举了个例子, 但是没看懂, 现在看懂了
![[Pasted image 20241022153916.png]]
向量从单数指向复数, $\vec{v}=plural-singular=\vec{A}-\vec{a}$, $\vec{v}\cdot\vec{A}=|A|^{2}-\vec{A}\cdot\vec{a}>\vec{A}\cdot\vec{a}-|a|^{2}$
后面这一点可从 $(\vec{A}-\vec{B})^{2}= A^{2}+B^{2}-2AB\cos\theta\ge0\rightarrow A^{2}+B^{2}\ge2Aa$ 
或者换个方式, $\vec{a}\cdot\vec{plur}<\vec{a}\cdot(\vec{a}+\vec{plur})=\vec{a}^{2}+\vec{a}\cdot\vec{plur}$
![[Pasted image 20241022160304.png]]
  训练时, 把这个嵌入矩阵分批作为训练数据丢进去, 批次大小为 Context size 上下文长度
  ```ad-col2
  ![[Pasted image 20241022160710.png]]
  
  ![[Pasted image 20241022162133.png]]
```

输出结果是一个预测列表, 大概 5 万个, 这里视频没说, 应该就是 50257 个   ![[Pasted image 20241022162403.png]]
然后经过 softmax 转换成概念列 ![[Pasted image 20241022162658.png]]
注意, 这里彩色矩阵是系数矩阵, 黑白色的是输入样本组成的矩阵, 这里是将最后一个 token 进行计算得到概率列, 这里这个系数矩阵叫解嵌入矩阵, 它的行数是 50257, 列数是 12288, 和嵌入矩阵是反过来的.
这里只对最后一个向量进行概率运算, 这一点现在不懂, 后面会讲 
![[Pasted image 20241022162956.png]]
Softmax 中有个参数 T, 调整它可以调整概率分布的输出, T 越大输出差距越小, 越小输出差距越大, 调整它可以调整精确模式和创造模式, 一般 API 不允许选择大于 2 的 T
```ad-col2
![[Pasted image 20241022163943.png]]

![[Pasted image 20241022164018.png]]
```
Softmax 的输入称为 logits![[Pasted image 20241022164253.png]]
## Chapter 6 Attention
初始的词嵌入与上下文无关, Attention 能计算出, 需要给初始的泛型嵌入加个什么向量才能把它移动到上下文对应的具体方向, 也就是计算出上文的 $\vec{plur}$
输入一大段文字, 输出却完全取决于最后一个单词, 最后一个单词经过所有注意力模块, 以包含远超单个词的信息量
 ```ad-col2
![[Pasted image 20241022165933.png]]

![[Pasted image 20241022165955.png]]
```
> [【官方双语】直观解释注意力机制，Transformer的核心 | 【深度学习第6章】\_哔哩哔哩\_bilibili](https://www.bilibili.com/video/BV1TZ421j7Ke?vd_source=56499cc54ebd02db0ac739e485d74801)
> 单头注意力机制从 5:00  开始

先问问题, 问题就是查询矩阵, 它把向量映射到一个低维查询空间中, 值矩阵就是回答,也把向量映射到同样维度的回答空间中, 回答查询的方式就是回答向量点乘查询矩阵
![[Pasted image 20241022181233.png]]
查询向量矩阵的计算由 $W_{Q}\cdot[E_{1}\quad E_{2}\cdots E_{50257}]=[Q_{1}\quad Q_{2}\cdots Q_{52057}]$, $Q_i$ 的维度等于 $W_{Q}$ 的维度
 ![[Pasted image 20241022182818.png]]
 每个查询向量要询问其他所有向量, 每 个回答向量要回复其他所有查询向量
 ![[Pasted image 20241022182938.png]]
  
每个 $Q$ 和 $K$ 的点乘都会得到一个分数, 可以看成相关程度. 比如说 $K_{1}$ 要回答所有 $Q$, 于是和所有 50256 个 $Q$ 点乘得到了 50256 个分数 $V$, 每个分数都可以看成 $E_{1}$ 和相应向量的相关程度, 于是得到第二个问题的回答时, $E_{1}$ 要迁移到 $E_{1}+V_{2}E_{2}$ 处, 得到第三个时, 在加 $V_{3}E_{3}$ 进行迁移, 最后 $E_{1}$ 迁移到了 $E_{1}+Score_{2}E_{2}+Score_{3}E_{3}+\cdots+Score_{50257}E_{50257}$ 处
![[Pasted image 20241022193321.png]]
上面这个例子只是为了理解, 实际上, $K_{1}$ 只需要回答它前面的 $Q$, 因为单词只应该受到前面单词的影响, 这样才符合预测的逻辑![[Pasted image 20241022195033.png]]
另外, 前面的 $V$ 不能直接用, $E_{1}$ 对应的 52056 个 $V$ 要经过 softmax 转变为概率列 ![[Pasted image 20241022194614.png]]
一般是设置为 $-\infty$, 使得左下为 $0$![[Pasted image 20241022195418.png]]
![[Pasted image 20241022194829.png]]
关于值矩阵 ![[Pasted image 20241022195645.png]]
所以, 前面例子中, $E_{1}$ 不是迁移到了$E_{1}+Score_{2}E_{2}+Score_{3}E_{3}+\cdots+Score_{50257}E_{50257}$ 处
而是 $E_{1}+Score_{2}V_{2}+\cdots+Score_{50257}V_{50257}$ 处, 用 $V_{2}$ 代替了原本的嵌入向量 $E_{2}$ 
- ? 为什么这里不直接用嵌入向量呢? 我猜可能是因为直接用嵌入向量的话结果就变成了嵌入向量的线性组合?
![[Pasted image 20241022200352.png]]
```ad-col2
![[Pasted image 20241022200702.png]]

![[Pasted image 20241022200803.png]]
```
一般把值矩阵写成两个矩阵相乘, 且中间维度一般设置成查询矩阵和键矩阵的维度 
- ? 承接上面那个疑问, 这里我可以看成, 先把嵌入向量映射到问题空间, 再升维回来, 这样才更能代表嵌入向量在这个问题下的信息
![[Pasted image 20241022201335.png]] 
###### 交叉注意力头 
18:40, 查询矩阵和键矩阵作用的数据集不同 ![[Pasted image 20241022201658.png]]
在这种情况下, 一般不会用到掩码使左下半为 0, 因为后面 token 可以影响前面的 token
###### 多头注意力
![[Pasted image 20241022202506.png]]
很好理解, 我们上面介绍的单头注意力只问了一个问题, 多头就是问多个问题
GPT-3 中有 96 个注意力头 ![[Pasted image 20241022202629.png]]
之前是 $E_{1}+Score_{2}V_{2}+\cdots+Score_{50257}V_{50257}=E_{1}+\Delta E_{1}$, 一个头给出了一个变化向量, 96 个头, 就会给出 96 个 ![[Pasted image 20241022202849.png]] ![[Pasted image 20241022202921.png]]
 这里注意之前的值矩阵是两个矩阵相乘得到的, 也就是说每个头都有一个值矩阵
 ![[Pasted image 20241022203105.png]]
 在代码中, 会把他们合起来, 并行计算更加高效
 第一个合在一起称为输出矩阵
![[Pasted image 20241022203204.png]]
想象一下, 第一个问题 $\Delta E_{1}=\sum_{i}^{25507}Score_{i}V_{1}$, 其中 $V_{i}=V^{\uparrow}V^{\downarrow}E_{1}$, 共 96 个问题
于是==把上图中所有的 $Value^{\downarrow}$ 竖着连起来==, 共 96 个, 连成一个矩阵, 行数为 128 x 96%%,(列数为 12288)%%,然后用 $E_{1}$ 左乘以它, 得到行数为 128 x 96 的输入矩阵, 而输出矩阵的列数为 128 x 96 %%(行数为 12288)%%, ==但是这里不是左乘输出矩阵, 而是输出矩阵沿着列数 128 切割, 输入矩阵沿着行数 128 切割==, 输出切割的第 i 个和输入切割的第 i 个相乘, 也就是 $V^{\uparrow}_{i}\cdot(V^{\downarrow}_{i}E_{1})=V_{i}$ ,最后得到 96 个 $V$, 第一个 $V_{1}$ 得到上面的 $\Delta E_{1}$, 一直到第 96 个
## 拓展
```ad-col2
![[Pasted image 20241022211307.png]]

![[Pasted image 20241022211246.png]]
```
