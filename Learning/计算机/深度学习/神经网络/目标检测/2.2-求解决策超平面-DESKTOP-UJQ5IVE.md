## 升维运算
对于这些在低维度下无法方便分类的数据，我们可以：
1. 通过合适的维度转换函数，将低维数据进行升维：![[Pasted image 20240827202432.png]]
2. 在高维度下求解 SVM 模型+：![[Pasted image 20240827202500.png]]
有一种方法，能够避免将数据送入高维度进行计算，又能获得同样的分类效果：
![[Pasted image 20240827202655.png]]
# 推导
![[Pasted image 20240827203218.png]]
 ![[Pasted image 20240827203421.png]]
 说明 $W$ 和决策超平面是互相垂直的，同时还满足 $\vec{w}\cdot(\vec{x_m}-\vec{x_n})=2$
  ![[Pasted image 20240827204043.png]]
  ![[Pasted image 20240827204138.png]]
   ![[Pasted image 20240827204310.png]]
   这等同于求解在约束条件下, 向量 $w$ 长度的最小化问题
   ![[Pasted image 20240827204541.png]]
   ![[Pasted image 20240827204646.png]]
- ? 凸优化： [zhuanlan.zhihu.com/p/85408804](https://zhuanlan.zhihu.com/p/85408804)
假设 $\vec{v_{1}}=(x_{11},x_{12},x_{13},\cdots,x_{1n})$ 为正超平面上一点, $\vec{v_{2}}=(x_{21},x_{22},x_{23},\cdots,x_{2n})$ 为负超平面上一点，$\vec{W}=(w_{1},w_{2},w_{3} ,\cdots,w_{n})$
有
$$
\begin{aligned}
\vec{W}\cdot\vec{v_{1}}+b=1&\qquad \vec{W}\cdot\vec{v_{2}}+b=-1\\
\vec{W}\cdot(&\vec{v_{1}}-\vec{v_{2}})=2
\end{aligned}
$$
又 $\vec{W}\cdot(\vec{v_{p1}}-\vec{v_{p2}})=0$, ($p_{1},p_{2}$ 为决策超平面上任意两点)，得 $\vec{W}\perp\vec{v_{p}}$  ，$\vec{W}$ 和 $L$ 同向
所以 $(\vec{v_{1}}-\vec{v_{2}})\cdot\frac{\vec{W}}{|\vec{W}|}=L$
所以 $(\vec{v_{1}}-\vec{v_{2}})\cdot\frac{\vec{W}}{|\vec{W}|}=2\cdot\frac{1}{|\vec{W}|}=L$
==所以 $\vec{W}$ 决定了 $L$ 的方向和大小==
目的是使 $L$ 最大，即 $|\vec{W}|$ 最小
由于样本点都满足 $y_{i}*(\vec{W}\cdot\vec{v}_{i}+b)\ge1$
设第 $i$ 个样本点满足 $y_{i}*(\vec{W}\cdot\vec{v}_{i}+b)-1=p_{i}^{2}$
共有 $s$ 个样本点，则有 $s$ 个约束条件，列出拉格朗日方程：
$$
L(\vec{w},b,\lambda_{i},p_{i})=\frac{\|\vec{w}\|^{2}}{2}-\sum_{i=1}^{s}\lambda_{i}*( y_{i}*(\vec{w}\cdot\vec{x}_{i}+b)-1-p_{i}^{2})
$$
其中 $s$ 为样本数量
## 分析拉格朗日方程
> [【数之道25】机器学习必经之路-SVM支持向量机的数学精华_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV13r4y1z7AG/?spm_id_from=333.788&vd_source=56499cc54ebd02db0ac739e485d74801)
![[Pasted image 20240828140457.png]]
$\lambda_i(y_i*(\vec{w}\cdot\vec{x}_i+b)-1)=0$ 说明两种情况：
1. 对应数据点不在正负超平面上，$\lambda=0$
2. 对应数据点在正负超平面上，$\lambda\ne0$
我们可以把拉格朗日乘子 $\lambda_i$ 看做违背约束条件的惩罚系数（❓）
当不满足约束条件时，括号内的值小于 0，（即 $y_{i}*(\vec{W}\cdot\vec{v}_{i}+b)<1$ ，$\vec{v_{i}}$ 在正负超平面之间）：![[Pasted image 20240830173759.png]]
此时如果 $\lambda_{i}<0$，则 $\lambda_{i}*( y_{i}*(\vec{w}\cdot\vec{x}_{i}+b)-1-p_{i}^{2})>0$ ，减去该项使得 $L(w,b,\lambda_{i},p_{i})$ 变得更小, $L$ 变得更大，==也就是说多了在正负超平面之间 的点，正负超平面间隔反而增加了==
![[Pasted image 20240830174742.png]]
![[Pasted image 20240830174759.png]]
+这是违反常理的
![[Pasted image 20240830174838.png]]
- 如果点不在正负超平面上，$\lambda_{i}=0$
- 如果点在正负超平面上，$\lambda_{i}\neq0$
- 如果点在正负超平面之间，即违反了决策，则 $\lambda_{i}\ge0$
## 可视化约束条件的影响
若只有一个约束条件
![[Pasted image 20240830233750.png]]
此时在切点处 $f(W)$ 的梯度向量指向其函数值增加最大的方向
![[Pasted image 20240830234030.png]]
![[Pasted image 20240830235208.png]] 
计算 $f(W)=|\vec{W}|^2$, 得梯度方向 $\vec{(w_{1},w_{2})}$, 注意此时 $(w_{1},w_{2})$ 是两者交点,
计算 $g_1(W,b)=y_1*(\vec{W}\cdot\vec{x}_1+b)-1\geq0$，这个函数中，$\vec{x_{1}}$ 才是系数，计算后直线的斜率为 $-\frac{x_{1}}{x_{2}}$, 梯度方向为 $(x_{1},x_{2})$, 相乘为 $-1$，梯度方向和直线方向垂直
- ! 看上去很奇怪，为什么 $g_{1}$ 的直线和梯度是垂直的呢？，可以假设 $g_1(W,b)=y_1*(\vec{W}\cdot\vec{x}_1+b)-1\geq0=p$, 想要让 $p$ 越大，即样本点离正负超平面越远，很明显沿着垂直于正负超平面的方向走是最快的，而这个方向和 $L、\vec{W}$ 的方向都是同一个方向 
![[Pasted image 20240831080412.png]]
$\lambda_{1}$ 调整为相同后满足 $\frac{\partial L}{\partial W}=\frac{\partial f}{\partial W}-\lambda_{1}\frac{\partial g_{1}}{\partial W}=0$
当第二个约束条件 $g_{2}\ge0$ 加入后 ![[Pasted image 20240831081449.png]]
此时 $f(w)$ 在最优解处的梯度向量方向,在两约束边界梯度向量包夹的区域内
![[Pasted image 20240831081800.png]]
因此可以由 $g_{1},g_{2}$ 的梯度向量分别乘以大于0的调整系数 $\lambda_{1}, \lambda_{2}$ 后相加组合而成
![[Pasted image 20240831082513.png]]
当第三个约束条件 $g_3\ge0$ 加入后，最优解已经由约束条件1，2在它们的约束边界交点处所决定，此时，最优解不在 $g_{3}$ 约束边界线上，约束条件不起实质作用
![[Pasted image 20240831083514.png]]
![[Pasted image 20240831160156.png]]
考虑 $(x_{32}w_{2}+x_{31}w_{1}+b)*y_{3}=1+p^2$，若 $y_{3}=1$，则
$$
\begin{align}
x_{32}w_{2}+x_{31}w_{1}+b-1-p^{2}&=0\\
\frac{|Ax_{0}+By_{0}+C|}{\sqrt{A^{2}+B^{2}}}=&\frac{|b-1-p^2|}{\sqrt{x_{32}^{2}+x_{31}^{2}}}
\end{align}
$$
若 $p^2$ 越大，则约束边界直线越靠近原点
- ! 不对，若 $b-1<0$，则 $p^{2}$ 越大越远离，下面也是，所以要看 $b$ 的大小, 但不管怎么样，$L$ 的偏导已经推断出不在正负超平面上得点的 $\lambda_{3}=0$
若 $y_{3}=-1$, 则考虑 $(x_{32}w_{2}+x_{31}w_{1}+b)*y_{3}=1+p^2$，因为它要小于-1
$$
\begin{align}
x_{32}w_{2}+x_{31}w_{1}+b&=-1-p^{2}\\
\frac{|Ax_{0}+By_{0}+C|}{\sqrt{A^{2}+B^{2}}}=&\frac{|b+1+p^2|}{\sqrt{x_{32}^{2}+x_{31}^{2}}}
\end{align}
$$
## 转换为对偶问题
现在，已经有 5 个条件了
![[Pasted image 20240831163532.png]]
不过在 SVM 中，为了求解的效率和更方便的应用核技巧（Kernel Trick），我们往往会将原问题转换为其自身的对偶问题，再求解+ ![[Pasted image 20240831163649.png]]
原方程有 $p$ 项，新方程没了
![[Pasted image 20240831164435.png]]
+同时 $f(W*)$ 是我们的极小值解，所以我们有
$$
q(\lambda_i)\leq f(\vec{w^*})-\sum_{i=1}^s\lambda_i*g_i(\vec{w^*},b^*)\leq f(\vec{w^*})\leq f(w)
$$
![[Pasted image 20240831165103.png]]
+那么我们可以去寻找一个最优下界 $q(\lambda_{i}^{*})$ （即 $q(\lambda_{i})$ 的最大值），让 $q(\lambda_{i}^{*})$ 与 $f(w^{*})$ 尽可能的接近+
![[Pasted image 20240831165520.png]]
这就是SVM原问题的对偶问题
![[Pasted image 20240831165637.png]]
当满足某些特定条件(Slater条件)，比如在当前仿射函数约束下求解凸优化问题时，强对偶成立。
而且我们可以证明当 $q(\lambda_{i}^{*})=f(w^{*})$ 时, 两者同时得到最优解 ![[Pasted image 20240831165959.png]]
![[Pasted image 20240831170116.png]]
==所以我们把求解 $f(W)=\sum^{n}_{i}w_{i}^{2}$ 的最小值转换为了求解 $q(\lambda_i)=q(W,b,\lambda)_{min}$ 的最大值==
### 简单可视化解释对偶性
```ad-col2
![[Pasted image 20240831171509.png]]

![[Pasted image 20240831171533.png]]
```
为了验证他的对偶性，我们构造 $q(\lambda)$
当我们调整 $\lambda$，我们可以清楚的看到拉格朗日方程 $x^{2}-\lambda(x-1)$ 的变化
```ad-col2
![[Pasted image 20240831171808.png]]

![[Pasted image 20240831171859.png]]
```
![[Pasted image 20240831172034.png]]
我们把不同 $\lambda$ 下的拉格朗日方程的最小值轨迹画出来，就得到了图中的曲线（下图黄色曲线） ![[Pasted image 20240831172127.png]]
这个最小值轨迹落在原问题极值下方，说明 $q(\lambda)$ 是原问题的下界 ![[Pasted image 20240831172556.png]]
我们对 $q(\lambda)$ 求最大值，得到 ![[Pasted image 20240831172701.png]]
![[Pasted image 20240831172910.png]]
很多时候对偶问题求解起来更方便，并且能表现出一些非常有用的特征
## 求解对偶问题
回到之前的问题，
![[Pasted image 20240831173028.png]]
![[Pasted image 20240831173057.png]]
$$
q(\lambda)=\min(\frac{\vec{W}^{2}}{2}-\sum^{s}_{i=1}\lambda_{i}(y_i*(\vec{W}\cdot\vec{x_i}+b)-1)))
$$
$$
\begin{align}
\frac{\partial L}{\partial W}=0&\longrightarrow \vec{W}-\sum_{i=1}^{s}\lambda_{i}y_{i}\vec{x_{i}}=0\rightarrow\vec{W}=\sum_{i=1}^{s}\lambda_{i}y_{i}\vec{x_{i}}\\
\frac{\partial L}{\partial \lambda}=0&\longrightarrow y_{i}*(\vec{W}\cdot\vec{x_i}+b)-1=0\\
\frac{\partial L}{\partial b}=0&\longrightarrow \sum^{s}_{i=1}\lambda_{i}y_{i}=0
\end{align}
$$
上式中
$$
\frac{\partial L}{\partial w_{1}}=0\longrightarrow w_{1}-\sum_{i=1}^{s}\lambda_{i}y_{i}\cdot x_{i1}=0\rightarrow w_{1}=\sum_{i=1}^{s}\lambda_{i}y_{i}\cdot x_{i1}
$$
$$
\begin{align}
w_{1}&=\lambda_{1}y_{1}\cdot x_{11}+\lambda_{2}y_{2}\cdot x_{21}+\cdots+\lambda_{s}y_{s}\cdot x_{s1}\\
w_{2}&=\lambda_{1}y_{1}\cdot x_{12}+\lambda_{2}y_{2}\cdot x_{22}+\cdots+\lambda_{s}y_{s}\cdot x_{s2}\\
\vdots\\
w_{n}&=\lambda_{1}y_{1}\cdot x_{1n}+\lambda_{2}y_{2}\cdot x_{2n}+\cdots+\lambda_{s}y_{s}\cdot x_{sn}\\
\end{align}
$$
即
$$
\begin{bmatrix}
w_{1} \\ w_{2} \\ \vdots \\ w_{n}
\end{bmatrix}
=\lambda_{1}y_1\vec{x_1}+\lambda_{2}y_2\vec{x_2}+\cdots+\lambda_{s}y_s\vec{x_s}
$$
联系之前的图片 ![[Pasted image 20240831081800.png]]
$\triangledown f=\lambda_{1}\triangledown g_{1}+\lambda_{2}\triangledown g_{2}=\lambda_{1}\vec{x_{1}}+\lambda_{2}\vec{x_{2}}$，结论是一致的
而 $\sum_{i=1}^{s}\lambda_{i}y_{i}=0$, 如图 ![[Pasted image 20240831225749.png]]
红点 $\lambda_{1}$ 为正，蓝点为负，蓝点的 $y=-1$ 使得蓝色向量转向，把 $y$ 为正和负的分离，得 $\sum\lambda_{红}=\sum\lambda_{蓝}$
，说明各个边界点对决策的影响是一致的，这里是二分类，若是 10 分类，则可能是 10 个点，$y_{1}$ 到 $y_{10}$ 可能是 1 到 10,1 到 10 的边界点，$\lambda_{1}*1+...+\lambda_{10}*10=0$

```ad-question
title: 疑点
collapse: open
上式中 $\frac{\partial L}{\partial \lambda}=0$ 指的是 $W^{*}$ 满足 $y_{i}*(\vec{W^{*}}\cdot\vec{x_i}+b)-1=0$, 可是 $W^*$ 并不能满足所有点 $X$ 都在正负超平面上，所以不可能使得所有 $x$ 都满足这个式子，那是怎么回事呢？
单独计算对 $\lambda$ 的偏导，有：
$$
\frac{\partial L}{\partial w_{i}}=y_{i}*(\vec{W}\cdot\vec{x_{i}}+b)-1=0
$$

则 $\vec{x_{i}}$ 在正负超平面上...

- & 不行，一团乱麻，待解决
```
- ~ 总之，上式中不必要对 $\lambda$ 求偏导，而且举的例子中也只求了 $\frac{\partial L}{\partial x}$ ，忽略了 $\frac{\partial L}{\partial \lambda}$，我猜可能是因为 $\lambda$ 是 $q(\lambda)$ 的自变量？
- ~ 不对不对，反转了，用到了对 $\lambda$ 的求偏导


所以
$$
\begin{align}
q(\lambda)&=\frac{(\sum_{i=1}^{s}\lambda_{i}y_{i}\cdot \vec{x_{i}})^2}{2}-\sum_{i=1}^{s}[\lambda_{i}y_i(\vec{W}\cdot\vec{x_{i}}+b)-\lambda_{i}y_{i}]\\
&=\frac{(\sum_{i=1}^{s}\lambda_{i}y_{i}\cdot \vec{x_{i}})^2}{2}-\sum_{i=1}^{s}\lambda_{i}y_i(\vec{W}\cdot\vec{x_{i}}+b)\\
&=\frac{(\sum_{i=1}^{s}\lambda_{i}y_{i}\cdot \vec{x_{i}})^2}{2}-\sum_{i=1}^{s}\lambda_{i}
\end{align}
$$
![[Pasted image 20240831232243.png]]
之所以我们绕了这样一大圈去求对偶问题,除了对偶问题从表达式和约束条件上更简洁以外,更主要的原因是因为对偶问题有一个非常好的特征,也就是最优解仅根据支持向量的点积结果所决定
![[Pasted image 20240831232438.png]]
推导点积：
由于$$
\begin{align}
w_{1}&=\lambda_{1}y_{1}\cdot x_{11}+\lambda_{2}y_{2}\cdot x_{21}+\cdots+\lambda_{s}y_{s}\cdot x_{s1}\\
w_{2}&=\lambda_{1}y_{1}\cdot x_{12}+\lambda_{2}y_{2}\cdot x_{22}+\cdots+\lambda_{s}y_{s}\cdot x_{s2}\\
\vdots\\
w_{n}&=\lambda_{1}y_{1}\cdot x_{1n}+\lambda_{2}y_{2}\cdot x_{2n}+\cdots+\lambda_{s}y_{s}\cdot x_{sn}\\
\end{align}
$$
所以
$$
\begin{align}
\begin{bmatrix}
(w_{1})^2 \\ (w_{2})^{2} \\ \vdots \\ (w_{n})^{2}
\end{bmatrix}
&=\left[\lambda_{1}y_1\vec{x_1}*\sum_{i=1}^{s}\lambda_{i}y_{i}\vec{x_{i}}\right]
+\left[\lambda_{2}y_2\vec{x_2}*\sum_{i=1}^{s}\lambda_{i}y_{i}\vec{x_{i}}\right]
+\cdots
+\left[\lambda_{s}y_{s}\vec{x_s}*\sum_{i=1}^{s}\lambda_{i}y_{i}\vec{x_{i}}\right]
\\
&=\sum_{i=1}^{s}\sum_{j=1}^{s}\lambda_{i} \lambda_{j}y_{i}y_{j}\vec{x}_{i}\cdot\vec{x}_{j}
\end{align}

$$
所以
$$
\begin{align}
q(\lambda)&=\frac{(\sum_{i=1}^{s}\lambda_{i}y_{i}\cdot \vec{x_{i}})^2}{2}-\sum_{i=1}^{s}\lambda_{i}\\
&=\frac{1}{2}\sum_{i=1}^{s}\sum_{j=1}^{s}\lambda_{i} \lambda_{j}y_{i}y_{j}\vec{x}_{i}\cdot\vec{x}_{j}-\sum_{i=1}^{s}\lambda_{i}
 \end{align}
$$ 
而且 $\lambda_{i}$ 当 $x_{i}$ 不为支持向量时为 0，所以计算 $\max q(\lambda)$ 时只需要用到支持向量
![[Pasted image 20240831232253.png]]
最优解仅根据支持向量的点积结果所决定，**也可以理解为仅由支持向量的空间相似度所决定**
**相似度越高，$\max q(\lambda)$ 就越大，$L$ 就越窄**

- @ 这么想一下，$\vec{W}$ 向量和决策超平面的方向垂直，支持向量的方向决定了它的类别，支持向量 $\vec{x}$ 和 $\vec{W}$ 相乘会得到一个数，该数的大小可以决定 $\vec{x}$ 的分类，
- @ $\vec{W}$ 决定于支持向量, **它由各个向量组成，每个向量都不可少，因此方向从各个分类的方向独立出来, ，以此作为一个分类的“标杆”**，但是，支持向量本身长度不一，所以要乘以 $\lambda$，减小长度对 $\vec{W}$ 的影响
- ! 超平面比它的法向量低一维，超平面是一个比它所在的空间维度少一维的子空间
![[Pasted image 20240901012829.png]]


- ? 既然我们只需要得到空间相似度结果，而这直接就可以通过核函数求得，那为什么还需要绕一圈去寻找维度转化函数呢？