---
aliases:
  - 2-SVM
tags: 
date created: Tuesday, August 27th 2024, 8:11:53 pm
date modified: Tuesday, August 27th 2024, 8:12:58 pm
---
>[【数之道】支持向量机SVM是什么，八分钟直觉理解其本质_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV16T4y1y7qj/?spm_id_from=333.337.search-card.all.click&vd_source=56499cc54ebd02db0ac739e485d74801)
# SVM
SVM，全称为**支持向量机**（Support Vector Machine），是一种用于分类和回归分析的监督学习模型。SVM 的基本思想是通过寻找一个最佳的超平面来将数据点分开，使得不同类别的样本之间的间隔（也就是 " 支持向量 "）最大化。
### SVM 的基本原理
- **超平面**：在 SVM 中，超平面是用来分割不同类别数据点的决策边界。在二维空间中，超平面就是一条线；在三维空间中，它是一个平面；在更高维的空间中，它就是一个超平面。
- **支持向量**：靠近决策边界的那些数据点，它们对确定超平面的位置起着关键作用。这些点称为支持向量。
- **间隔**：支持向量到超平面的距离称为间隔（Margin）。SVM 的目标是最大化这个间隔，即找到一个使得各类数据点分开最远的超平面。
- **软间隔与硬间隔**：在实际问题中，数据往往是不可完全线性分割的。SVM 允许在分类过程中有一定的错误分类，这就是软间隔。而硬间隔则要求数据完全线性可分。
### 核函数（Kernel Function）
当数据在原始空间中不可线性分割时，可以通过引入核函数将数据映射到更高维度的特征空间中，使其在高维空间中线性可分。常见的核函数包括：
- 线性核（Linear Kernel）
- 多项式核（Polynomial Kernel）
- 径向基函数核（RBF Kernel）
- Sigmoid 核
### 应用
SVM 在处理小规模、线性可分的数据集时表现得非常出色，并且在高维数据集上具有较强的鲁棒性。它被广泛应用于图像分类、文本分类、生物信息学等领域。
### 优缺点
**优点**：
- 能够有效处理高维数据。
- 在小样本情况下表现良好。
- 支持使用不同的核函数来处理非线性分类问题。
**缺点**：
- 对于大规模数据集，计算复杂度较高。
- 在处理非线性数据时，核函数的选择至关重要。
- 对于含有噪声的数据，SVM 的表现可能不如其他一些模型
# 决策边界
**决策边界**是指在分类问题中，用于将不同类别的样本分隔开的边界或界线。它是分类器在输入空间中的一个分割面，能够将数据点按照所属的类别进行区分。
### 决策边界的特点
- **线性决策边界**：如果决策边界是一条直线（在二维空间中）或者一个平面（在三维空间中），那么这个分类器就是线性的。例如，线性支持向量机（SVM）的决策边界就是一个线性超平面。
- **非线性决策边界**：如果决策边界不是一条直线或平面，而是曲线或复杂形状的边界，则分类器是非线性的。例如，使用径向基函数（RBF）核的 SVM 的决策边界通常是非线性的。
### 决策边界的作用
决策边界的作用是将特征空间划分成不同的区域，每个区域对应一个类别。对于一个给定的样本，当我们将其特征值输入到分类器中时，分类器会根据这个样本在特征空间中的位置来判断它落在决策边界的哪一侧，从而确定它的类别。
### 举例说明
- **线性决策边界**：假设我们有两类数据点，分别为红色和蓝色。如果这两类数据在二维平面上可以被一条直线完全分开，那么这条直线就是决策边界。所有在直线一侧的点都会被分类为红色，另一侧的点则被分类为蓝色。
- **非线性决策边界**：如果红色和蓝色的数据点是环形分布的，那么线性决策边界（直线）无法很好地将它们分开。此时，可以通过某种非线性变换将数据点映射到一个更高维的空间，在这个高维空间中用超平面来分开数据点。这对应的非线性决策边界在原始空间中可能是一个圆形或者更复杂的曲线。
### 决策边界的重要性
决策边界的形状和位置直接影响分类器的性能。在训练分类模型时，目标通常是找到一个合适的决策边界，使得分类错误率最小。因此，理解和构建合适的决策边界是设计和优化分类器的重要部分。
# 核函数
我们来通过一个简单的数学例子，结合 SVM 和决策边界的概念，来解释如何通过引入核函数将数据从原始空间映射到高维空间中，使其线性可分。
### 问题描述
假设我们有一个二维平面上的二分类问题，数据点分别属于两类，红色和蓝色。数据分布如下：
- 红色类：以原点为圆心，半径为 1 的圆内的数据点。
- 蓝色类：以原点为圆心，半径为 2 的圆上的数据点。
在这个二维空间中，红色和蓝色的点分布无法通过一条直线（线性决策边界）来完全分开。
### 数据点表示
假设红色点的坐标为 $(x_1, y_1)$，蓝色点的坐标为 $(x_2, y_2)$。
红色类的条件是： $$x_1^2 + y_1^2 \leq 1$$
蓝色类的条件是： $$1 < x_2^2 + y_2^2 \leq 4$$
### 线性不可分
在二维空间中，无论如何画一条直线，都无法将这两类数据点完全分开。因此，传统的线性分类器在这种情况下表现不佳。
### 引入核函数
为了将这些数据点线性分开，我们可以引入一个核函数，将数据点从二维空间映射到更高维度的特征空间中。例如，我们可以考虑一个简单的核函数：
$$ \phi(x, y) = (x^2, y^2, \sqrt{2}xy) $$
这个核函数将原来的二维数据点 $(x, y)$ 映射到一个三维空间中的点 $(x^2, y^2, \sqrt{2}xy)$。
### 高维空间中的线性分割
在新的三维空间中：
- 红色点满足 $x_1^2 + y_1^2 \leq 1$，它们在这个新空间中的表示为 $(x_1^2, y_1^2, \sqrt{2}x_1y_1)$。
- 蓝色点满足 $1 < x_2^2 + y_2^2 \leq 4$，它们在这个新空间中的表示为 $(x_2^2, y_2^2, \sqrt{2}x_2y_2)$。
在三维空间中，我们可以使用一个超平面（即一个平面）来将这两类数据点分开。例如，超平面方程可以是：
$$ z = x^2 + y^2 - 1 $$
这个平面可以将红色点和蓝色点完全分开，因此在三维空间中，这些数据点是线性可分的。
### 结论
通过引入核函数并将数据点映射到更高维空间，我们能够找到一个线性决策边界（超平面）来分开原本在低维空间中线性不可分的数据。这就是核函数和决策边界结合的数学原理。