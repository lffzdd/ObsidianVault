![[Pasted image 20240827203218.png]]
通过推导，我已经知道 $Ax+By+C=0$ 的法向量是 $(A,B)$，因为 $x$ 每走 $B$ 步，$y$ 就走 $-A$ 步，所以斜率为 $\frac{-A}{B}$, 法向量与它互相垂直
设 $f(x,y)=w_{1}x+w_{2}y=\vec{W}\cdot\vec{(x,y)}$ ，明显它是**线性**的
所以，对两个边界点 $(x_{1}, y_{1})$ 和 $(x_{2},y_{2})$, 有
$$
\begin{align}
w_{1}x_{1}+w_{2}y_{1}+b=A\\
w_{1}x_{2}+w_{2}y_{2}+b=B
\end{align}
$$
假设 $w_{1}x+w_{2}y\ge A$ 为类别 1，$w_{1}x+w_{2}y\le B$ 为类别 2
但是若 $w_{1}x+w_{2}\in(A, B)$ ，这样的样本点就无法分类了，所以得定义一个值，$f(x,y)$ 大于这个值，就为分类 1，小于这个值，就为分类 2。自然地，这个值为 $A$ 和 $B$ 的平均值更合适
那么可以认为
$$
w_{1}x+w_{2}y+b=\frac{A+B}{2}
$$
是分类边界，$w_{1}x+w_{2}y>\frac{A+B}{2}$ 为类别 1，$w_{1}x+w_{2}y<\frac{A+B}{2}$ 为类别 2
我们可以把 $f(x,y)=A,B,\frac{A+B}{2}$ 这三个边界缩放到：
$$
\begin{align}
w_{1}\frac{x_{1|old}}{A}+w_{2}\frac{y_{1|old}}{A}+\frac{b_{old}}{A}=w_{1}x_{1}+w_{2}y_{1}+b&=1\\
w_{1}\frac{x_{1|old}}{-B}+w_{2}\frac{y_{1|old}}{-B}+\frac{b_{old}}{-B}=w_{1}x_{2}+w_{2}y_{2}+b&=-1\\
w_{1}x_{2}+w_{2}y_{2}+b&=0
\end{align}
$$

![[Pasted image 20240901133847.png]]
如图，蓝色为 $\vec{(x_{1},y_{1})}$, 绿色为 $\vec{(x_{2},y_{2})}$, 红色为 $\vec{W}$, 既然 $\vec{W}\cdot\vec{(x_{1},y_{1})}=1$, 所以 $|\vec{W}|\cdot L=2$。
![[Pasted image 20240901171326.png]]
黑色为 $\vec{v_{1}}-\vec{v_{2}}$, $L$ 最大的时候就是这个长度，黑色向量会垂直于超平面
我们知道，
$$
\vec{W}=\lambda_{1}y_1\vec{v_{1}}+\lambda_{2}y_2\vec{v_{2}}+\cdots+\lambda_{s}y_{s}\vec{v_{s}}
$$
在这里是，
$$
\vec{W}=\lambda_{1}\vec{v_{1}}-\lambda_2\vec{v_2}
$$
要找到 $\lambda_{1},\lambda_{2}$ 使得 $y_{1}\vec{v_{1}}$ 和 $y_{2}\vec{v_{2}}$ 的线性组合为 $\vec{W}$，即方向和大小都相同
或者说, $\vec{W}$ 以 $y_{1}\vec{v_{1}},y_{2}\vec{v_{2}}$ 为基向量，要找到坐标 $(\lambda_{1},\lambda_{2})$ 使得 $\vec{W}$ 的模长最小，而 $\vec{W}$ 的方向和 $\vec{v_{1}}-\vec{v_{2}}$ 的方向是一样的

- ! 要说明的一点是，不管是几维空间，SVM 从原理上都是二分类模型
```ad-attention
title: SVM是二分类模型
collapse: closed
**是的，不管几维空间，SVM（支持向量机）从原理上都是二分类模型**。SVM的基本思想是通过找到一个最优的决策边界（超平面）来分隔两个类别的数据点。在二维空间中，这个决策边界表现为一条直线；在三维空间中，表现为一个平面；而在更高维度的空间中，则表现为一个超平面。尽管空间维度可能不同，但SVM的核心目标始终是通过最大化不同类别数据点到这个超平面的间隔来找到一个最优的分类边界。

然而，尽管SVM本质上是二分类模型，但它可以通过一些策略扩展到多分类问题。这些策略包括一对一（One-vs-One, OVO）和一对多（One-vs-Rest, OVR）方法。在OVO方法中，对于N个类别的数据，需要构建N(N-1)/2个二分类器，每个分类器用于区分两个类别。而在OVR方法中，则构建N个二分类器，每个分类器将其中一个类别与其他所有类别区分开来。通过这些方法，SVM可以在多分类问题上取得良好的表现。

总的来说，不管在几维空间中，SVM的核心原理都是基于二分类的，但它可以通过组合多个二分类器来解决多分类问题。

```
