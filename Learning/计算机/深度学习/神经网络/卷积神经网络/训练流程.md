---
aliases: 训练流程
tags: []
date created: Invalid date
date modified: 2024, 六月 21日, 06:32:00,  星期五晚上
---
# 整体流程
分为 epochs 个周期，每个周期进行$M=m/batch\_size$次训练，每次训练针对一个批次，每经过一次训练参数更新一次，然后进行下一个批次的训练，每一次训练都是从神经网络的输入层到输出层，5 个周期中，总遍历次数为$M \times 5 = 5M$次。
在每个批次中，神经网络会进行前向传播、计算损失、反向传播和参数更新。这样，每次处理一个批次时，模型参数会更新一次。因此，5 个周期中，神经网络的参数将更新$5M$次
若输入 256 张 28 x 28 x 3 的图片，epochs=5，batch_size=64，
第一卷积层有 32 个卷积核，卷积核大小为 3 x 3 x 3
第二层有 64 个卷积核，卷积核大小为 3 x 3 x 3
则第一次训练，每个卷积核有 3×3×3=273 个权重参数，加上一个偏置参数，总共有 27+1=28 个参数，32 个卷积核的总参数数量为 32×28=896。
$$
32个卷积核
\begin{cases}
\begin{bmatrix}  
\phantom{x} |& \phantom{x} |& \\  
\hline % 注意：这不是LaTeX标准的矩阵分隔线，仅用于示意  
\phantom{x} |& \phantom{x} |& \\  
\hline  
\phantom{0} |& \phantom{0} |&\\  
\end{bmatrix}\rightarrow64张特征图 \cases{26\times26\times3\rightarrow\begin{bmatrix}
\end{bmatrix}\\\vdots\\\vdots}\\
\vdots \\
\vdots \\
第32个卷积核
\end{cases}
$$
### 损失计算
假设我们有一个简单的分类任务，使用交叉熵损失函数。假设真实标签$\mathbf{Y}_{\text{true}}$形状为$(64, \text{num\_classes})$，这里的$\text{num\_classes}$是分类的类别数量。
损失函数定义为：
$$L = -\frac{1}{m} \sum_{i=1}^{m} \sum_{c=1}^{C} y_{i, c} \log(\hat{y}_{i, c})$$
其中：
- $m$ 是批次大小（64）
- $C$ 是类别数量
- $y_{i, c}​$ 是第 $i$ 个样本第 $c$ 类的真实标签
- $\hat{y}_{i, c}$ ​ 是第 $i$ 个样本第 $c$ 类的预测值
# 数据转换
```Python
images = data.reshape((10000,3,32,32)).transpose(0,2,3,1)
```
CIFAR-10 数据集里的 `data` 数组每一行是按通道存储的，每个通道的所有像素值依次排列。因此，每一行应该是 `[r1, r2, r3, …, r1024, g1, g2, …, g1024, b1, …, b1024]`。
第一个 32 是高度，将三行的每一行切分成 32 块
# 第一层的输出
好的，结合矩阵运算，详细解释卷积层的操作步骤如下：
### 输入数据
输入数据是64张32x32x3的图片，表示为一个四维张量，形状为：
$(64, 32, 32, 3)$
这里，64是批量大小，32x32是每张图片的高度和宽度，3是通道数（颜色通道：红色、绿色和蓝色）。
### 卷积层参数
卷积层定义如下：
```python
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
```
- **滤波器数量**：32
- **滤波器大小**：3x3
- **激活函数**：ReLU
- **输入形状**：32x32x3
### 卷积操作
#### 单通道卷积
首先考虑单通道的情况。一个3x3的滤波器在单通道输入上的卷积操作可以表示为：
输入矩阵（假设为一个单通道的5x5矩阵，示意用）：
$$X = \begin{bmatrix}
x_{11} & x_{12} & x_{13} & x_{14} & x_{15} \\
x_{21} & x_{22} & x_{23} & x_{24} & x_{25} \\
x_{31} & x_{32} & x_{33} & x_{34} & x_{35} \\
x_{41} & x_{42} & x_{43} & x_{44} & x_{45} \\
x_{51} & x_{52} & x_{53} & x_{54} & x_{55}
\end{bmatrix}$$
3x3滤波器矩阵（示意用）：
$$W = \begin{bmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33}
\end{bmatrix}$$
卷积操作在输入矩阵上滑动，计算点积，生成输出矩阵（假设无填充和步幅为1）：
输出矩阵的一个元素的计算过程：
$Y_{11} = x_{11}w_{11} + x_{12}w_{12} + x_{13}w_{13} + x_{21}w_{21} + x_{22}w_{22} + x_{23}w_{23} + x_{31}w_{31} + x_{32}w_{32} + x_{33}w_{33}$
#### 多通道卷积
对于多通道输入，每个滤波器在每个通道上执行卷积操作，**然后将结果求和**。
输入矩阵（32x32x3，表示为三个32x32的矩阵堆叠）：
$$X^1 = \begin{bmatrix}
x_{11}^1 & x_{12}^1 & \cdots & x_{132}^1 \\
x_{21}^1 & x_{22}^1 & \cdots & x_{232}^1 \\
\vdots & \vdots & \ddots & \vdots \\
x_{321}^1 & x_{322}^1 & \cdots & x_{3232}^1
\end{bmatrix}$$
$$X^2 = \begin{bmatrix}
x_{11}^2 & x_{12}^2 & \cdots & x_{132}^2 \\
x_{21}^2 & x_{22}^2 & \cdots & x_{232}^2 \\
\vdots & \vdots & \ddots & \vdots \\
x_{321}^2 & x_{322}^2 & \cdots & x_{3232}^2
\end{bmatrix}$$
$$X^3 = \begin{bmatrix}
x_{11}^3 & x_{12}^3 & \cdots & x_{132}^3 \\
x_{21}^3 & x_{22}^3 & \cdots & x_{232}^3 \\
\vdots & \vdots & \ddots & \vdots \\
x_{321}^3 & x_{322}^3 & \cdots & x_{3232}^3
\end{bmatrix}$$
每个滤波器也有三个3x3的核：
$$W^1 = \begin{bmatrix}
w_{11}^1 & w_{12}^1 & w_{13}^1 \\
w_{21}^1 & w_{22}^1 & w_{23}^1 \\
w_{31}^1 & w_{32}^1 & w_{33}^1
\end{bmatrix}$$
$$W^2 = \begin{bmatrix}
w_{11}^2 & w_{12}^2 & w_{13}^2 \\
w_{21}^2 & w_{22}^2 & w_{23}^2 \\
w_{31}^2 & w_{32}^2 & w_{33}^2
\end{bmatrix}$$
$$W^3 = \begin{bmatrix}
w_{11}^3 & w_{12}^3 & w_{13}^3 \\
w_{21}^3 & w_{22}^3 & w_{23}^3 \\
w_{31}^3 & w_{32}^3 & w_{33}^3
\end{bmatrix}$$
每个滤波器在三个通道上卷积后结果求和得到一个输出矩阵：
$Y_{ij} = \sum_{c=1}^3 \sum_{u=1}^3 \sum_{v=1}^3 x_{i+u-1, j+v-1}^c \cdot w_{uv}^c$
通过上述计算，得到一个输出通道。
### 多个滤波器
因为我们有32个滤波器，所以会得到32个这样的输出矩阵，每个矩阵的大小为30x30。即每个滤波器会产生一个30x30的特征图。
### 激活函数
经过ReLU激活函数，输出的每个值被处理为：
$\text{ReLU}(x) = \max(0, x)$
### 最终输出形状
将这些操作应用于每个输入图片，我们的输出数据形状为：
$(64, 30, 30, 32)$
其中：
- 64：批量大小
- 30：输出的高度
- 30：输出的宽度
- 32：滤波器数量（通道数）
综上所述，输入的64张32x32x3图片经过卷积层后的输出是64张30x30x32的特征图。
# 第二层的输出
当第二层是 `layers.Conv2D(64, (3, 3), activation='relu')`，它接受第一层的输出（64, 30, 30, 32）作为输入时，我们可以通过以下步骤详细说明输出的形状和计算过程。
### 输入数据形状
第一层的输出是64张30x30x32的特征图，形状为：
$(64, 30, 30, 32)$
这里，64是批量大小，30x30是每张特征图的高度和宽度，32是通道数（滤波器数量）。
### 第二层卷积层参数
卷积层定义如下：
```python
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
```
- **滤波器数量**：64
- **滤波器大小**：3x3
- **激活函数**：ReLU
### 卷积操作
#### 单通道卷积
对于每个输入通道，每个3x3的滤波器在单个通道上进行卷积操作。假设无填充（valid padding）和步幅为1。
输入矩阵（30x30，表示为一个单通道）：
$$X = \begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{130} \\
x_{21} & x_{22} & \cdots & x_{230} \\
\vdots & \vdots & \ddots & \vdots \\
x_{301} & x_{302} & \cdots & x_{3030}
\end{bmatrix}$$
3x3滤波器矩阵：
$$W = \begin{bmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33}
\end{bmatrix}$$
卷积操作在输入矩阵上滑动，计算点积，生成输出矩阵：
输出矩阵的一个元素的计算过程：
$Y_{11} = x_{11}w_{11} + x_{12}w_{12} + x_{13}w_{13} + x_{21}w_{21} + x_{22}w_{22} + x_{23}w_{23} + x_{31}w_{31} + x_{32}w_{32} + x_{33}w_{33}$
#### 多通道卷积
对于多通道输入，每个滤波器在每个通道上执行卷积操作，然后将结果求和。
输入矩阵（30x30x32，表示为32个30x30的矩阵堆叠）：
$X^1, X^2, \ldots, X^{32}$
每个滤波器也有32个3x3的核（对应32个输入通道）：
$W^1, W^2, \ldots, W^{32}$
每个滤波器在32个通道上卷积后结果求和得到一个输出矩阵：
$Y_{ij} = \sum_{c=1}^{32} \sum_{u=1}^3 \sum_{v=1}^3 x_{i+u-1, j+v-1}^c \cdot w_{uv}^c$
通过上述计算，得到一个输出通道。
### 多个滤波器
因为我们有64个滤波器，所以会得到64个这样的输出矩阵，每个矩阵的大小为28x28。即每个滤波器会产生一个28x28的特征图。
### 激活函数
经过ReLU激活函数，输出的每个值被处理为：
$\text{ReLU}(x) = \max(0, x)$
### 最终输出形状
综合以上计算，最终输出数据的形状为：
$(64, 28, 28, 64)$
### 数学总结
- 输入形状： (64, 30, 30, 32)
- 卷积滤波器数量： 64
- 卷积滤波器大小： 3x3
- 步幅： 1
- 填充： 无（valid padding）
- 输出形状计算：
  - 高度和宽度：$\left\lfloor \frac{30 - 3}{1} \right\rfloor + 1 = 28$
  - 通道数：64（滤波器数量）
所以，经过这层卷积层后的输出形状为$(64, 28, 28, 64)$，表示64张28x28的特征图，每张图有64个通道。
总权重数量=3×3×32×64
此外，每个滤波器还有一个偏置项，总共有64个滤波器，所以偏置项的数量是64，总计 18496 个参数
# 全连接层的输出
如果下一层是全连接层（Dense层），则需要先将卷积层的输出展平成一维。
### 展平操作
假设我们将卷积层的输出形状 `(64, 28, 28, 64)` 展平为一维向量。展平操作会将每张图片的每个特征图的所有像素值按顺序排列成一个长向量。
对于每张输入图片，卷积层的输出形状是 `(28, 28, 64)`。我们需要将其展平成一个一维向量，具体步骤如下：
1. **输出特征图形状**： `(28, 28, 64)`
2. **展平后的向量长度**：
$28 \times 28 \times 64 = 50176$
因此，每张图片的特征图展平后得到一个长度为 50176 的向量。
### 输入全连接层
假设下一层是一个全连接层：
```python
model.add(layers.Dense(128, activation='relu'))
```
该层有128个神经元，每个神经元都连接到展平后的所有输入特征。
### 全连接层的参数数量
对于全连接层，每个神经元的参数数量等于输入特征的数量加一个偏置项。因此，全连接层的参数数量计算如下：
1. **输入特征数**：50176
2. **神经元数**：128
每个神经元的权重数量：
$\text{每个神经元的权重数} = 50176$
总权重数量：
$\text{总权重数量} = 50176 \times 128 = 6422528$
加上偏置项的数量，每个神经元一个偏置项，总共有128个偏置项。
总参数数量：
$\text{总参数数量} = 6422528 + 128 = 6422656$
### 总结
如果将卷积层输出 `(64, 28, 28, 64)` 展平为一维向量，并输入到一个全连接层 `Dense(128, activation='relu')`，需要进行以下操作：
1. 展平每张图片的输出特征图，从 `(28, 28, 64)` 展平为长度为 50176 的一维向量。
2. 全连接层会有 50176 个输入特征，并连接到128个神经元。
全连接层的总参数数量为 6422656（包括权重和偏置项）。
---
全连接层（Dense层）的输出形状可以通过以下步骤确定：
### 输入到全连接层
假设卷积层的输出形状为 `(64, 28, 28, 64)`，我们先将其展平成一维向量。每张输入图片的特征图展平后的向量长度为：
$28 \times 28 \times 64 = 50176$
因此，每张图片展平后的形状为 `(50176,)`。
### 全连接层定义
假设我们有一个全连接层：
```python
model.add(layers.Dense(128, activation='relu'))
```
### 全连接层的输出形状
全连接层有128个神经元，每个神经元接收50176个输入特征，并产生一个输出值。因此，经过全连接层后的输出形状为：
\[ (64, 128) \]
其中：
- 64：批量大小
- 128：全连接层的神经元数量
### 输出具体解释
1. **批量大小**：输入的64张图片的批量大小保持不变。
2. **每张图片的特征**：每张图片经过全连接层后，从展平的一维向量（长度50176）转换为一个128维的向量。
### 数学计算
1. **展平操作**：
   - 输入形状： `(64, 28, 28, 64)`
   - 展平后形状： `(64, 50176)`
2. **全连接层**：
   - 输入特征数：50176
   - 神经元数：128
3. **全连接层输出**：
   - 每张图片的输出：一个128维向量
   - 总输出形状： `(64, 128)`
### 总结
经过全连接层 `Dense(128, activation='relu')`，每张展平后输入向量（长度50176）会被转换为一个128维的向量。因此，输入批量为64的图片经过全连接层后的输出形状为 `(64, 128)`。