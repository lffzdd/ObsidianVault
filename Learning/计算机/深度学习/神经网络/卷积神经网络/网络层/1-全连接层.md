---
aliases: 1-全连接层
tags:
date created: Sunday, June 30th 2024, 5:43:21 pm
date modified: Thursday, August 8th 2024, 6:03:12 pm
---
# 全连接层

> 全连接层（Fully Connected Layer，简称 FC 层）是神经网络中常见的一种层类型，它将输入的所有神经元与输出的所有神经元完全连接起来。每一个输入神经元都连接到每一个输出神经元，并通过加权求和和非线性激活函数来进行计算。

### 全连接层的工作原理
#### 1. 输入和输出
假设输入向量的大小为 n，输出向量的大小为 m。输入向量可以表示为 $\mathbf{x} = [x_1, x_2, \ldots, x_n]$，输出向量可以表示为 $\mathbf{y} = [y_1, y_2, \ldots, y_m]$。
#### 2. 权重和偏置
全连接层有两个参数矩阵：权重矩阵 $\mathbf{W}$ 和偏置向量 $\mathbf{b}$。
- **权重矩阵**：大小为 $m \times n$，表示每个输入神经元与每个输出神经元之间的连接权重。权重矩阵可以表示为：
  $$\mathbf{W} = \begin{bmatrix} w_{11} & w_{12} & \cdots & w_{1n} \\ w_{21} & w_{22} & \cdots & w_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ w_{m1} & w_{m2} & \cdots & w_{mn} \\ \end{bmatrix}$$
- **偏置向量**：大小为 $m$，表示每个输出神经元的偏置项。偏置向量可以表示为：
  $$\mathbf{b} = [b_1, b_2, \ldots, b_m] $$
#### 3. 计算过程
全连接层的计算可以表示为：
$$\mathbf{y} = f(\mathbf{W} \cdot \mathbf{x} + \mathbf{b})$$
其中，$f$ 是激活函数，常用的激活函数有 ReLU、sigmoid、tanh 等。
### 计算示例
假设有以下参数：
- 输入向量 $\mathbf{x} = [x_1, x_2, x_3]$
- 权重矩阵 $\mathbf{W} = \begin{bmatrix} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \end{bmatrix}$
- 偏置向量 $\mathbf{b} = [b_1, b_2]$
- 激活函数 $f$ 是 ReLU（Rectified Linear Unit）
计算过程如下：
1. **矩阵乘法**：
   $$\mathbf{z} = \mathbf{W} \cdot \mathbf{x} = \begin{bmatrix} w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} w_{11}x_1 + w_{12}x_2 + w_{13}x_3 \\ w_{21}x_1 + w_{22}x_2 + w_{23}x_3 \end{bmatrix}$$
2. **加上偏置**：
   $$\mathbf{z} = \mathbf{z} + \mathbf{b} = \begin{bmatrix} w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + b_1 \\ w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + b_2 \end{bmatrix}$$
3. **应用激活函数（ReLU）**：
   $$\mathbf{y} = f(\mathbf{z}) = \begin{bmatrix} \max(0, w_{11}x_1 + w_{12}x_2 + w_{13}x_3 + b_1) \\ \max(0, w_{21}x_1 + w_{22}x_2 + w_{23}x_3 + b_2) \end{bmatrix}$$
- & 若是 10 分类问题，则 $\mathbf{W}$ 有十行，每一行对应一个分类，例如 0-9 数字分类，每一行对应一个数字
