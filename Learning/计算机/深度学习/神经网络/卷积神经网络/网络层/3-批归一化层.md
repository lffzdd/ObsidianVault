---
aliases: 3-批归一化层
tags: []
date created: Invalid date
date modified: 2024, 七月 6日, 11:44:54,  星期六晚上
---
# 理论
### 为什么批归一化能加速训练过程和稳定训练
**批归一化（Batch Normalization）**是对神经网络进行归一化的一种方法，目的是减少内部协变量偏移（Internal Covariate Shift），从而加速训练过程并提高模型的稳定性。
#### 数学原理
批归一化的主要步骤如下：
1. **计算小批次的均值和方差**：对于小批次中的每一个特征$x$，计算均值$\mu_B$和方差$\sigma_B^2$。
2. **归一化**：利用均值和方差对每一个特征进行归一化：
$$
   \hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$
   其中，$\epsilon$是一个很小的数，用于防止分母为零。
3. **缩放和平移**：引入可学习的参数$\gamma$和$\beta$，进行缩放和平移：
  $$
   y = \gamma \hat{x} + \beta
  $$
   这样使得模型有能力恢复数据的分布。
#### 批归一化加速训练过程的原因
1. **减小内部协变量偏移**：内部协变量偏移是指在训练过程中，前一层参数的变化会影响到后面层的输入分布。批归一化通过使每一层的输入数据均值为 0，方差为 1，来减少这种影响，从而使得每一层的输入分布更加稳定。
2. **使损失函数更平滑**：批归一化后，每一层的输入分布更加稳定，使得损失函数更平滑，优化器可以更容易地找到全局最优解。
3. **使梯度更陡峭**：由于输入数据的分布更加稳定，梯度下降过程中的梯度变化更加一致，从而使得梯度更加陡峭，优化器可以沿着更陡峭的路径进行更新，加速收敛。
#### 举例说明
假设我们有一个深层神经网络，在没有批归一化的情况下，每一层输入数据的分布可能会发生较大变化，这会导致每一层的权重需要不断调整来适应新的输入分布，从而使得训练过程变得缓慢且不稳定。
引入批归一化后，每一层的输入数据的分布变得更加一致，损失函数的曲面变得更加平滑。优化器可以沿着更加一致和陡峭的梯度进行更新，加速了收敛过程。
#### 结论
通过批归一化，神经网络可以更快、更稳定地进行训练，因为它减少了内部协变量偏移，使得损失函数更加平滑，并使梯度更加陡峭。这样优化器能够更有效地找到全局最优解，提高了训练速度和稳定性。
# GPT 问答
是的，您理解得非常正确。具体来说，在批次数据形状为$(30, 32, 32, 64)$的情况下，批量归一化的计算步骤如下：
1. **每个通道独立计算**：
   - 对于第一个通道（第一个特征图），计算该通道在所有 30 张图像中的所有像素的均值和方差。
   - 对于第二个通道（第二个特征图），计算该通道在所有 30 张图像中的所有像素的均值和方差。
   - 以此类推，直到第 64 个通道（第 64 个特征图）。
2. **计算均值和方差**：
   - 对于每个通道$c$，计算整个批次中该通道的所有像素的均值：
    $$
     \mu_c = \frac{1}{N \times H \times W} \sum_{i=1}^{N} \sum_{j=1}^{H} \sum_{k=1}^{W} x_{i,j,k,c}
    $$
     其中$N = 30$，$H = 32$，$W = 32$。
   - 对于方差$\sigma_c^2$：
    $$
     \sigma_c^2 = \frac{1}{N \times H \times W} \sum_{i=1}^{N} \sum_{j=1}^{H} \sum_{k=1}^{W} (x_{i,j,k,c} - \mu_c)^2
    $$
3. **标准化每个通道的数据**：
   - 对于每个通道$c$，使用均值$\mu_c$和方差$\sigma_c$将数据标准化：
    $$
     \hat{x}_{i,j,k,c} = \frac{x_{i,j,k,c} - \mu_c}{\sqrt{\sigma_c^2 + \epsilon}}
    $$
     其中$\epsilon$是一个小值，用来防止除零。
4. **缩放和平移**（可选）：
   - 通过可学习的参数$\gamma_c$和$\beta_c$对标准化的数据进行缩放和平移：
    $$
     y_{i,j,k,c} = \gamma_c \hat{x}_{i,j,k,c} + \beta_c
    $$
### 关键点总结
- **每个通道独立处理**：每个通道独立计算均值和方差，因此总共计算 64 个均值和 64 个方差。
- **基于整个批次的计算**：均值和方差是基于整个批次的所有图像来计算的，而不是每张图像单独计算。
这就是为什么批量归一化可以在训练过程中加速和稳定模型，因为它在批次内进行归一化，从而减小了内部协变量转移的影响。
# 代码
```python
keras.layers.BatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer="zeros",
    gamma_initializer="ones",
    moving_mean_initializer="zeros",
    moving_variance_initializer="ones",
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    synchronized=False,
    **kwargs
)
```
Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.
批量归一化应用一种变换，使平均输出保持接近 0，输出标准差接近 1。
Importantly, batch normalization works differently during training and during inference.
重要的是，批量归一化在训练和推理期间的工作方式不同。
**During training** (i.e. when using `fit()` or when calling the layer/model with the argument `training=True`), the layer normalizes its output using the mean and standard deviation of the current batch of inputs. That is to say, for each channel being normalized, the layer returns `gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta`, where:
在训练期间（即使用 `fit()` 或使用参数 `training=True` 调用层/模型时），层使用当前批次输入的平均值和标准差标准化其输出。也就是说，对于每个被归一化的通道，该层返回 `gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta` ，其中：
- `epsilon` is small constant (configurable as part of the constructor arguments)
    `epsilon` 是小常量（可配置为构造函数参数的一部分）
- `gamma` is a learned scaling factor (initialized as 1), which can be disabled by passing `scale=False` to the constructor.
    `gamma` 是一个学习的缩放因子（初始化为 1），可以通过将 `scale=False` 传递给构造函数来禁用它。
- `beta` is a learned offset factor (initialized as 0), which can be disabled by passing `center=False` to the constructor.
    `beta` 是一个学习的偏移因子（初始化为 0），可以通过将 `center=False` 传递给构造函数来禁用它。
在推理过程中（即使用 `evaluate()` 或 `predict()` 或使用参数 `training=False` （默认值）调用层/模型时，该层标准化其输出使用训练期间看到的批次的平均值和标准差的移动平均值。也就是说，它返回 `gamma * (batch - self.moving_mean) / sqrt(self.moving_var+epsilon) + beta` 。
`self.moving_mean` and `self.moving_var` are non-trainable variables that are updated each time the layer in called in training mode, as such:
`self.moving_mean` 和 `self.moving_var` 是不可训练的变量，每次在训练模式下调用层时都会更新这些变量，如下所示：
- `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`
- `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`
As such, the layer will only normalize its inputs during inference _after having been trained on data that has similar statistics as the inference data_.
因此，该层仅在接受与推理数据具有相似统计数据的数据训练后，才会在推理过程中对其输入进行归一化。
**Arguments 论点**
- **axis**: Integer, the axis that should be normalized (typically the features axis). For instance, after a `Conv2D` layer with `data_format="channels_first"`, use `axis=1`.
    axis：整数，应该标准化的轴（通常是特征轴）。例如，在带有 `data_format="channels_first"` 的 `Conv2D` 层之后，使用 `axis=1` 。
- **momentum**: Momentum for the moving average.
    动量：移动平均线的动量。
- **epsilon**: Small float added to variance to avoid dividing by zero.
    epsilon：添加到方差中的小浮点以避免被零除。
- **center**: If `True`, add offset of `beta` to normalized tensor. If `False`, `beta` is ignored.
    center：如果 `True` ，则将 `beta` 的偏移量添加到归一化张量。如果 `False` ，则忽略 `beta` 。
- **scale**: If `True`, multiply by `gamma`. If `False`, `gamma` is not used. When the next layer is linear this can be disabled since the scaling will be done by the next layer.
    比例：如果 `True` ，乘以 `gamma` 。如果 `False` ，则不使用 `gamma` 。当下一层是线性时，可以禁用此功能，因为缩放将由下一层完成。
- **beta_initializer**: Initializer for the beta weight.
    beta_initializer：beta 权重的初始化器。
- **gamma_initializer**: Initializer for the gamma weight.
    gamma_initializer：伽玛权重的初始化器。
- **moving_mean_initializer**: Initializer for the moving mean.
    moving_mean_initializer：移动平均值的初始化器。
- **moving_variance_initializer**: Initializer for the moving variance.
    moving_variance_initializer：移动方差的初始化器。
- **beta_regularizer**: Optional regularizer for the beta weight.
    beta_regularizer：beta 权重的可选正则化器。
- **gamma_regularizer**: Optional regularizer for the gamma weight.
    gamma_regularizer：伽玛权重的可选正则化器。
- **beta_constraint**: Optional constraint for the beta weight.
    beta_constraint：beta 权重的可选约束。
- **gamma_constraint**: Optional constraint for the gamma weight.
    gamma_constraint：伽玛权重的可选约束。
- **synchronized**: Only applicable with the TensorFlow backend. If `True`, synchronizes the global batch statistics (mean and variance) for the layer across all devices at each training step in a distributed training strategy. If `False`, each replica uses its own local batch statistics.
    同步：仅适用于 TensorFlow 后端。如果 `True` ，则在分布式训练策略中的每个训练步骤中跨所有设备同步该层的全局批次统计信息（均值和方差）。如果 `False` ，每个副本都使用自己的本地批次统计信息。
- **`**kwargs`**: Base layer keyword arguments (e.g. `name` and `dtype`).
    `**kwargs`：基础层关键字参数（例如 `name` 和 `dtype` ）。
