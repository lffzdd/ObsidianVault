
# 什么是 n-gram 模型？
**n-gram 模型** 是自然语言处理中（NLP）一种非常基础的模型，主要用于建模一段文本中单词出现的**概率**。
简单来说，**n-gram 是指连续出现的 n 个词**。
- 当 n=1 时，叫 **unigram（一元语法）**，只看一个词。
- 当 n=2 时，叫 **bigram（二元语法）**，看连续两个词。
- 当 n=3 时，叫 **trigram（三元语法）**，看连续三个词。
- 以此类推。
### 为什么要用 n-gram？
我们希望**预测一句话中下一个词出现的概率**。  
比如输入 "我爱"，你想知道下一个词是 "你"、"北京"、"学习" 的概率哪个更高。
如果你用 n-gram 模型（比如 bigram），那么：
- 只需要看前一个词（"爱"）来预测下一个词。
这种方法简单又有效，尤其在数据量不大的时候。
### n-gram 怎么建模？
核心是根据**统计出现的频率**来估计概率。
举个简单例子：  
假设你的训练数据是这几句话：
```
我 爱 北京
我 爱 你
你 爱 北京
```
**统计 bigram（二元语法）**：

| 上一个词 | 当前词 | 次数  |
| :--: | :-: | :-: |
|  我   |  爱  |  2  |
|  爱   | 北京  |  2  |
|  爱   |  你  |  1  |
|  你   |  爱  |  1  |
**如何计算概率？**
比如你要算 `P(北京 | 爱)`（在“爱”之后是“北京”的概率）：
公式是：
$$P(当前词 | 上一个词) = \frac{上一个词和当前词一起出现的次数}{上一个词出现的总次数}$$
所以，
$$P(北京 | 爱) = \frac{2}{3} \quad (因为爱后面出现了3次：2次是北京，1次是你)$$
### 总结一下 n-gram 的流程：
1. 统计 n-gram 出现的频率。
2. 根据频率计算条件概率。
3. 用条件概率预测下一个词。
### 小问题：n-gram 有什么缺点？
- **数据稀疏**：你没见过的词组合，模型不会。
- **短视问题**：只能看到固定 n 个词，前面的内容记不住太多。
- **需要平滑**（smoothing）：为了避免没见过的组合概率变成 0。
（如果你感兴趣，我可以继续教你什么是**平滑**技术，比如加一平滑。）
数学原理
好的！现在我们开始**深入讲 n-gram 的数学原理**。  
## 1. 目标：建模一个句子的概率
在 NLP 中，我们希望为一句话$w_1, w_2, w_3, \ldots, w_T$ 计算它的整体概率：
$$P(w_1, w_2, w_3, \ldots, w_T)$$
根据**链式法则（Chain Rule of Probability）**，我们可以把整体概率拆成一系列条件概率：
$$P(w_1, w_2, \ldots, w_T) = P(w_1) \times P(w_2|w_1) \times P(w_3|w_1, w_2) \times \cdots \times P(w_T|w_1, \ldots, w_{T-1})$$
就是说：
- 第一个词的概率 P(w_1)
- 第二个词在第一个词给定的情况下的概率 P(w_2|w_1)
- 第三个词在前两个词给定的情况下的概率 P(w_3|w_1, w_2)
- ……
## 2. 问题：太复杂了
如果一句话很长，比如 10 个词，要计算：
$$P(w_{10} | w_1, w_2, \ldots, w_9)$$
前面有 9 个条件词，太难了！几乎统计不到这么多种组合，需要巨大数据量。
## 3. 解决办法：马尔可夫假设（Markov Assumption）
于是我们做一个**简化假设**：
> 未来只依赖于最近的几个词，而不是全部历史。
如果是 **n-gram** 模型，我们只看前 n−1n-1 个词。
比如在 **bigram** (n=2) 里面，我们假设：
$$P(w_t|w_1, w_2, \ldots, w_{t-1}) \approx P(w_t|w_{t-1})$$
只依赖上一个词！
如果是 **trigram** (n=3)：
$$P(w_t|w_1, w_2, \ldots, w_{t-1}) \approx P(w_t|w_{t-2}, w_{t-1})$$
只依赖前两个词！
## 4. 怎么估计概率？
**通过统计频率（Maximum Likelihood Estimation, MLE）**！
公式：
对于 n-gram：
$$P(w_t | w_{t-n+1}, \ldots, w_{t-1}) = \frac{Count(w_{t-n+1}, \ldots, w_{t-1}, w_t)}{Count(w_{t-n+1}, \ldots, w_{t-1})}$$
意思是：
- 分子：统计 (n 个词) 连在一起出现的次数
- 分母：统计 (前 n-1 个词) 连在一起出现的次数
**例子：**
如果你想算：
P(北京∣我,爱)P(\text{北京}|\text{我}, \text{爱})
- 分子：统计 "我 爱 北京" 出现了几次
- 分母：统计 "我 爱" 出现了几次
然后用 分子 ÷ 分母。
## 5. 具体例子：Bigram 详细推导
给定语料：
```
我 爱 北京
我 爱 你
你 爱 北京
```
统计：
- Count(我)=2Count(\text{我}) = 2
- Count(我, 爱)=2Count(\text{我, 爱}) = 2
- Count(爱, 北京)=2Count(\text{爱, 北京}) = 2
- Count(爱, 你)=1Count(\text{爱, 你}) = 1
那么：
- P(爱∣我)=Count(我, 爱)Count(我)=22=1P(\text{爱}|\text{我}) = \frac{Count(\text{我, 爱})}{Count(\text{我})} = \frac{2}{2} = 1
- P(北京∣爱)=Count(爱, 北京)Count(爱)=23P(\text{北京}|\text{爱}) = \frac{Count(\text{爱, 北京})}{Count(\text{爱})} = \frac{2}{3} （因为爱后面有两次北京，一次你，总共3次）
# 6. 总结一下：
- **链式法则** 分解整体概率。
- **马尔可夫假设** 降低了条件数量，只依赖最近的 n−1n-1 个词。
- **MLE** 通过数数来估计每个条件概率。
- 最后，乘起来得到整句的概率。
要不要我再继续讲讲：
- 平滑（Smoothing）：比如有些组合没出现过，怎么办？
- 训练 n-gram 的小代码示例？
- n-gram 和神经网络语言模型的区别？
你想听哪部分？😄（可以选一个继续）